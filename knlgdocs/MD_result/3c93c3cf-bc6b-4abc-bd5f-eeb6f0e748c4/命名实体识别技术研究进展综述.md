![](images/0ff857b7efaf1a53b555644dc7ef61d8550e5160e37cf754abe710ed8e95bdea.jpg)

中图分类号：TP391.1文献标志码：A文章编号:2095-641X(2022)02015-1DOl:10.16543/2095-641x.electrc.power.ic.202.02.003著录格式：江千军,桂前进,王磊,等.命名实体识别技术研究进展综述[J].电力信息与通信技术,2022,20(2):15-24.

# 命名实体识别技术研究进展综述

江千军1,桂前进1,王磊²,徐瑞翔‘，王京景²,麦立²,许水清（1.国网安徽省电力有限公司安庆供电公司，安徽安庆 246000；2.国网安徽省电力有限公司，安徽 合肥 230009;3.合肥工业大学 电气与自动化工程学院，安徽 合肥 230009)

# A Review of the Research Progress of Named Entity Recognition

JIANG Qianjun', GUI Qianjing', WANGLei², XU Ruixiang', WANG Jingjing², MAILi², XU Shuiqing (1. Anqing Power Supply Company, State Grid Anhui Electric Power Co., Ltd., Anqing 246000, China; 2. State Grid Anhui Electric Power Co., Ltd., Hefei 230009, China; . Collge ofElectrical Engineering and Automation,Hefei University of Technology, Hefei 230009, China)

摘要：命名实体识别是指从文本中提取出专有名词或特定命名实体的识别任务，作为信息抽取中非结构化数据转化为结构化数据的关键步骤，在机器翻译、情感分析、信息检索等领域有广泛应用，是自然语言处理的热点问题。文章就现有的命名实体识别技术方法进行了详细地梳理，重点阐述了基于深度学习的命名实体识别方法及其实现过程，进而分析了具有代表性的典型算法的优缺点，并对命名实体识别技术在电力系统中的应用前景进行了展望。

关键词：命名实体识别；深度学习；迁移学习；注意力模型

Abstract: Named entity recognition refers to the task of extracting proper nouns or specific named entities from text. As a key step of information extraction to transform unstructured data into structured data, it has ben widely applied in machine translation, emotion analysis, information retrieval and other fields, which has been a hot issue in natural language processing. This paper makes a detailed analysis of the existing named entity recognition technology methods,focuses on the deep learning based on named entity recognition method and its implementation process. Furthermore, it analyzes the advantages and disadvantages of the representative typical algorithms, meanwhile, discussng the application prospect of named entity recognition in power system.

![](images/ac24d6107d86cdb5906105988e6bebe2c691c3995935f923ff175a1417e13603.jpg)

Key words: named entity recognition; deep learning; transfer learning; attention model

# 0 引言

随着科技的飞速发展，计算机数据呈爆炸式增长。虽然这些海量的原始数据蕴含丰富的信息，但大部分是以非结构化的文本数据存储，不能被计算机直接利用，造成很多有效信息难以提取，无法在数据库中交互流通。而自然语言处理（Natural LanguageProcessing，NLP）作为完成人和计算机之间自然语言通信的各种技术和理论方法，其目标是使计算机能够充分理解并自主学习自然语言[1-2],主要包括3 部分：分词和词性标注、命名实体识别、实体关系抽取。其中命名实体识别（Name Entity Recognition,NER）是知识库问答系统、机器翻译、信息检索、情感分析、知识图谱等多项自然语言处理应用的基础任务，其目的是从文本中抽取出具有一定意义的名词或短语的识别任务，且识别结果作为实体关系抽取的基础[3]。

为了不断提升实体识别效果，命名实体识别技术一直是重要且富有挑战性的研究课题。基于词典和规则的方法是最早提出的命名实体识别方法，伴随机器学习的广泛使用，许多专家提出了各种各样的基于统计机器学习的命名实体识别方法。当前，随着人工智能技术的日新月异，基于深度学习的命名实体识别方法成为目前应用最广泛的方法[4-6]。

![](images/075466f5376334aea3023f2f3dd6d936ba70bed73ad0e51883c1c920c20ac9ba.jpg)

虽然现阶段提出了许多命名实体识别方法，然而由于命名实体自身的随意性、复杂性、多变性等特点，命名实体识别仍有许多待解决的问题。本文在这些研究成果的基础上，梳理了现有的命名实体识别技术方法，详细综述了基于深度学习的命名实体识别的国内外最新研究进展，探讨了命名实体识别技术目前存在的问题，展望了命名实体识别技术在电力系统中的应用前景。

# 传统命名实体识别技术

命名实体识别是自然语言处理的一项基础任务，最初是 在 MUC-6(The Sixth of the Series MessageUnderstandingConference）上提出的，当时研究的核心是如何在一些领域进行信息抽取，如从报章等非结构化文本中抽取关于机构活动的人名、地名、组织机构名等核心结构化信息。随后在MUC-7中，命名实体类别被进一步分为多类，即三大类(实体类、时间类、数字类）和七小类（人名、机构名、地名、时间、日期、货币和百分比)[7-9]。从最初的词典和规则的方法，发展到统计机器学习方法，一直到最近应用范围最广的基于深度学习的模型，命名实体识别技术一直是自然语言处理领域中的热点研究方向。

# 1.1 基于词典和规则的实体识别技术

早期的实体识别技术主要通过文本与模型一一匹配的方式完成识别任务，主要包括2种方法：基于词典的实体识别和基于规则的实体识别。基于词典的实体识别方法是指词典中的每个词与被处理文档之间逐一匹配的过程。基于规则的实体识别方法是根据文本特点与定制规则特点匹配的方式完成实体识别，常用的定制方式包含：根据命名实体的构词规则、根据匹配标点符号的规则、根据命名实体和上下文词性用词特点的规则等[10-12]。

基于词典和规则的实体识别方法使用简单，结果准确率较高，特别是对于数字和时间日期实体利用规则匹配的方式能获得较好的识别效果。但是词典和规则库的建立需要花费大量时间和人力；不同的实体类型需要定制相应的规则，移植性差。为了解决上述问题，一些专家学者研究了统计机器学习的实体识别方法[13]。

# 1.2 基于统计机器学习的实体识别技术

![](images/dd64b69671c5ea44d32ac0238c4adf700948e16db1844e4eadcbb3c25b683f5f.jpg)

基于统计机器学习的方法是从给定的、已标注好的训练集出发，通过人工构建特征，并根据特定的模型对文本中每个词进行标签标注，实现命名实体识别。

在基于机器学习的命名实体识别方法中，标注的词语通常使用IOBES 标注集表示（见表1),即每个词可以用表1中前5类标签进行分类标注。因此基于机器学习的方法也称为序列标注法，即对输入文本材料中的每个字词进行自动序列化标注，再将标注的标签进行整合，最终获得有类别标注的命名实体。

表1IOBES标签标注集  
Table 1 Label annotation set of IOBES   

<table><tr><td>字母</td><td>含义</td></tr><tr><td>b</td><td>Begin,表示一个词的开头</td></tr><tr><td>i</td><td>Intermediate,表示词的中间部分</td></tr><tr><td>e</td><td>End,表示词的结尾</td></tr><tr><td>0</td><td>Other,表示无关字符</td></tr><tr><td>s</td><td>Single,表示单个字符</td></tr><tr><td>PEr</td><td>人名</td></tr><tr><td>LOC</td><td>地名</td></tr><tr><td>ORG</td><td>组织机构名</td></tr></table>

典型的基于统计机器学习的实体识别技术有隐马尔可夫模型（Hidden Markov Model,HMM）、最大熵马尔可夫模型（Maximum Entropy Markov Model,MEMM）、支持向量机（Support Vector Machine,SVM）模型、条件随机场（ConditionalRandomFields，CRF）模型。

HMM是最早提出的基于统计机器学习的实体识别技术，结构简单，训练效率高，但该算法并不能考虑上下文信息。在HMM基础上发展而来的MEMM解决了HMM输出独立性的问题，并具有较为灵活的特征选取方式，但因为其局部归一化计算而出现了标签偏置问题。SVM算法相对于其他3 种方法较为简单，降低了计算维数，但对参数和核函数选择要求高，且不适用于训练大规模样本。CRF是通过一组已知的输入随机变量来推导另一组输出随机变量的条件概率分布模型，能利用所有已知信息求得标签序列，并且条件随机场模型可以有效地避免出现最大熵马尔可夫模型中的标记偏置和隐马尔可夫模型识别复杂命名实体难度大的问题，具有较好的识别性能，但相比其他3种方法，CRF训练时间较长。4种基于统计机器学习的实体识别技术对比见表2所列。

![](images/ef0c3df525b103f61c32c6b45f438eb6bc7e63fd216b8609cbe1d54c1ad472b6.jpg)

表24种基于统计机器学习的实体识别技术对比 Table 2 Four entity recognition technologies based on statistical machine learning   

<table><tr><td>模型</td><td>优势</td><td>不足</td></tr><tr><td></td><td>隐马尔可夫模型 适合小数据集，训练效率较高 不能考虑上下文的特征</td><td></td></tr><tr><td>模型</td><td>最大熵马尔可夫 特征设计灵活,解决了HMM 输出独立性的问题</td><td>局部归一化，标签偏置</td></tr><tr><td>支持向量机模型</td><td>算法简单,有较好的鲁棒性</td><td>对参数和核函数选择要 求高</td></tr><tr><td>条件随机场模型</td><td>可利用上下文信息,全局归一 化求得最优解</td><td>收敛速度慢、复杂度高</td></tr></table>

基于统计机器学习算法的命名实体识别模型对特征选取的要求较高，并且需要丰富的语料库。因此，有学者提出将词典规则与统计机器学习相结合的模型：先建立对应的词典或规则模板，提取出对应特征，再利用统计机器学习完成标签序列标注，从而获得较高的召回率和准确率[14]。如Wang等[15]提出了一种基于词典和CRF结合的细菌命名实体识别模型：根据指定的实体类别在大量有关微生物文献上人工构建标注语料库获得细菌名实体词典，结合已知标注语料和定制特征训练得到一个能自主识别实体的词典，即条件随机场命名实体识别模型。

结合词典规则的统计机器学习模型适用于专业性比较强的领域，可在一定程度上提高分词的准确性，但词典和规则的训练语料库构建程序繁琐，需要爬取大量数据库。同时基于统计的机器学习算法依赖于人工定制特征，限制了该方法在仅有少量标注数据集范围中的更深入使用。

# 2　基于深度学习的命名实体识别技术

近年来，命名实体识别技术普遍使用基于多层神经网络的深度学习（DeepLearning，DL）模型。深度学习可以看作是一种由多层神经网络构成的机器学习算法[16-20]。不同于基于统计的机器学习方法，深度学习本身包含较强的泛化能力，可以从原始数据中自行获取特征，不依赖于专家知识和人工特征。深度学习主要包括深度神经网络模型、注意力模型和迁移学习模型。深度神经网络模型的核心思想是将输入的词编码成实数向量，利用该实数向量通过网络输出层将单词映射到标签空间，实现分类函数。常见的深度网络模型网络结构包括：循环神经网络（RecurrentNeural Network,RNN）、长短期记忆网络（LongShort-TermMemory，LSTM）、门控循环单元（GatedRecurrent Unit，GRU）及其相应的改进模型[21-25]。与深度神经网络模型相比，迁移学习适用于标注数据稀缺的情况。注意力模型通过相似度计算，在各神经网络单元引入权重系数，提升模型识别精度。

# 2.1循环神经网络模型

循环神经网络（RNN）是最早提出的基于深度学习的命名实体识别方法，RNN含有一条数据通路指向自身，使得在同一个层面的神经单元之间有了关联（见图1)，RNN的这种结构使其更适用于解决时间序列问题。

![](images/9df47f804486020c9db738fb7c9bdc647ede4da0bbf18aa1fc0da64a27bc1318.jpg)  
图1RNN结构Fig.1 The structure of RNN

图中： $X$ 表示输入层的值；Y表示输出层的值； $U$ 是输入值 $X$ 的权重矩阵; $V$ 是输出层 $Y$ 的权重矩阵;$W$ 是隐藏层的权重矩阵。

循环神经网络中的神经单元的计算是按时间顺序展开进行的过程。前时刻神经单元通过数据通路与后时刻神经单元进行连接，单元之间的数据从一层流向下一层，记忆信息保留的程度通过网络参数进行控制。数据通过重复链接可以在它的网络结构内循环，这样使得RNN含有记忆功能，在上下文联系的自然语言处理或时间序列的文本特征问题的处理中具有一定的优势，如Hu等[26]在CCKS2017 测评任务上提出一个基于规则、CRF、RNN 和带特征RNN的混合系统，并在系统最后添加一个投票机制，当且仅当候选实体被至少2种方法选中时，它才会被选择为命名实体。

然而，随着序列长度的不断增加，RNN会逐步丧失学习能力，这种现象被称为“梯度消失”，因此

![](images/d62e390078a063ef8d1b52a549cd990127d54f1fb4f4a731a969c33effb89008.jpg)

RNN无法进行长时间序列标注。针对基于RNN在训练中容易发生梯度爆炸和梯度消失问题[27-28],提出了由RNN改进而来的长短期记忆网络。

# 2.2 长短期记忆网络及其改进模型

长短期记忆网络（LSTM）是由Hochreiter等[29]于1997年提出，LSTM通过门结构设计可以选择性地记录或遗忘历史信息，避免了RNN梯度爆炸、梯度消失以及距离依赖问题。

LSTM结构如图2所示，一个LSTM神经单元结构包含更新门(输入门）、忘记门、输出门。每个门的计算过程都有当前输入信息和上一时刻的神经单元状态参与，并经过激活函数tanh的非线性映射计算。在 $t$ 时刻，LSTM的输入分别有上一时刻LSTM的隐藏层输出值 $h _ { t - 1 } ,$ 当前时刻网络的输入值 $x _ { t }$ ，以及上一时刻的单元状态值 $c _ { t - 1 } 3$ 部分组成，LSTM的输出有当前时刻LSTM输出值 $h _ { t }$ 和当前时刻的单元状态 $c _ { t } 2$ 部分组成。其中更新门决定当前时刻模型对于神经元记忆状态的输入更新程度；忘记门则决定前一时刻的神经元记忆状态在当前时刻的遗忘程度;输出门决定更新后的神经元记忆状态输出程度。通过以上3个门结构的设计，LSTM在一定程度上解决了梯度消失和梯度爆炸问题。

![](images/e449bfc60cae051375ccba9f588d64698b4515847dda6033ed6a6a733687d8c8.jpg)  
图2LSTM结构  
Fig.2 The structure of LSTM

然而单向的LSTM网络只包含一层正向隐藏层，仅能利用输入序列的单向信息。为了获得更好的识别效果，Graves 等[30]提出利用双向长短期记忆网络（Bi-LSTM）来充分获取正反2个方向信息以取得较优的编码序列。Bi-LSTM可以同时学习双向序列信息，并且避免了RNN存在的梯度消失问题，可以存储长时间的信息，便于模型更好的完成命名实体识别任务。Habibi等[31]提出一种面向生物医学领域的LSTM-CRF命名实体识别模型。在这种方法中，把词向量和字符级向量进行组合作为LSTM-CRF 的输入向量。在此基础上，文献[32]进一步提出目前应用最为广泛的实体识别方法之一：Bi-LSTM-CRF模型（见图3）。该方法大致流程是：将文本用字／词向量形式表示,完成字符嵌入；使用深度神经网络有监督地训练模型，识别实体类型并进行标签标注；标注序列优化，解决标签无序性问题。流程中利用word2vec 进行字符嵌入，Bi-LSTM网络进行信息学习，CRF使用动态规划算法找出最优标注序列[33-34]。

![](images/1ba443cdd905e8cb8656f69436e9b5d4eccd49d496fead879d8bc3751e0fa063.jpg)

![](images/355850ca3519d7df528007a73d4b2075e6fb571f73a26428ee4825f382805eb4.jpg)  
图3 基于Bi-LSTM-CRF模型的命名实体识别方法 Fig.3 The NER method based on Bi-LSTM-CRF

# 2.3门控循环单元及其改进模型

门控循环单元（GRU）是为了解决LSTM神经网络结构复杂、训练时间过长而提出的，结构如图4所示，GRU网络结构将LSTM的3个门进行整合后分为重置门和更新门。其中，重置门的作用是控制前一时刻隐藏层状态信息被忽略的程度，更新门的作用是控制前一时刻的隐藏层状态信息被带入到当前隐藏层状态中的程度。

![](images/de26f29b3fbf526f5532b6f87ca3451951f8732150db0e26768bd065b4484d4e.jpg)  
图4 GRU结构Fig.4 The structure of GRU

Jozefowicz等[35]对GRU模型进行实验,验证了虽然GRU减少了网络参数量，然而模型整体性能并没有受到干扰。这表明GRU在处理长时间序列问题时又可以较好的减少训练时间，加快了模型收敛速度。

![](images/244d1b469f6fbebc892ddd5b947133565871e2d4d4684c8729e68aa59dd3a22d.jpg)

在 GRU的基础上，石春丹等[36]提出了一种基于双向门控循环单元的命名实体识别模型BGRU-CRF。该模型利用外部数据集预训练词嵌入词典，将隐藏词特征加入到基于字符的BGRU-CRF 模型中，充分利用文本语义信息有效地避免了命名实体歧义;同时该模型加入注意力机制来设置BGRU网络中特定信息的权重，通过选择文本中最关键的字词提升命名实体识别效果。Yang等[37]提出了一种基于深度门控单元网络的序列标注模型，利用BI-GRU网络结构提取出含有输入文本语义特征的字、词拼接向量，并利用CRF实现最后的标签预测，在此基础上，通过共享网络结构和参数提升模型泛化性，实现跨语言和多任务的联合训练。

# 2.4 迁移学习模型

为了解决基于有监督学习的命名实体识别模型需要大量标记数据的问题，有学者提出了新的机器学习模型——迁移学习（Transfer Learning）[38-40]模型，结构如图5所示，核心思想是运用已标注数据集解决不同但相关目的领域的问题，核心是探索把已标注的数据集源领域（sourcedomain）知识迁移到将学习的新数据集目标领域（targetdomain）上。这2个领域之间有一定的相关性，利用相关性将源领域已学习数据应用到目标领域中就是迁移学习的过程。源领域与目标领域数据集的相似程度越高，迁移学习效果越好。迁移学习解决了命名实体识别已有标注数据不足的问题，自提出后便在自然语言处理领域得到了广泛的应用[41-43]。

![](images/b81c5e7fd969ae29b4a6099a8416f7ce93b97f328ce0f3fdd3a5e157ac29b914.jpg)  
图5迁移学习模型结构

在小规模标注语料的情况下，深度迁移学习模型利用多层非线性神经网络自主学习实体特征，通过迁移学习完成从源领域到目标领域的特征迁移，有效缓解了深度学习在少量数据时学习能力不够的问题，深度挖掘神经网络的特征提取功能，降低了对人工特征的依赖。Gligic等[44]针对标注电子健康记录数据稀缺的情况，将未注释的电子记录进行预训练得到词向量，通过迁移学习训练神经网络。Xu等[45] 针对中文社交媒体的命名实体识别提出了一种可以从域外标记数据集和域内未标注数据集学习的联合模型。联合模型能够进行根据领域相似度跨领域学习和自训练的半监督学习，有效地缓减了模型训练标注数据集受限的问题。武惠等[46]针对中文机构命名实体识别问题,提出了一种基于深度迁移学习的命名实体识别模型TrBiLSTM-CRF,采用基于实例的迁移学习算法，通过生成权值和选择样本来增加具有正迁移特性的训练样本，降低了具有负迁移特性或零迁移特性的样本对训练模型的影响。Kim等[47]提出一种基于远程监督的深度迁移学习命名实体识别模型。利用远程监督获得的初代模型参数初始化子模型并用少量标注数据微调模型，完成命名实体识别模型的训练。远程监督和迁移学习结合的方法在利用远程监督节省标注数据成本的同时，通过迁移学习剔除数据噪声有效提高了模型识别准确度。王银瑞等[48]提出一种基于迁移学习的中文命名实体识别模型Trans-NER，模型结构如图6所示。

![](images/81edf6f35ca960afe665452c372d57fccc3edbfed9b667e51f4059f127d5df33.jpg)

![](images/7fb620b39285b98783e6a8567522f5d365c7b0a630f7fd7b632ac2359cbb2a94.jpg)  
Fig.5 The model structure of transfer learning   
图6Trans-NER 模型结构  
Fig.6 The model structure of Trans-NER

迁移学习能够利用源领域的标注数据将共性特征迁移至新兴的、缺乏标注语料的目标领域，有效解决了有监督学习依赖数据集的问题，提高了模型的可移植性。迁移学习为更多领域提供了基础信息支持，能够在少量人工标注语料上获得较好的命名实体识别结果，实现对现有数据充分利用。借鉴迁移学习的方法，可以在命名实体识别任务上进行更深入的研究。

![](images/054de2aeabd4ce3d8ae6a1dc2cf16b2ae3d9285ae46df386e76e226c276a376d.jpg)

# 2.5 注意力模型

注意力机制源于机器模仿人类对关键事物会更加注意的视觉特点，核心思想是通过增加权重来突出关键词，减小权重剔除不必要的部分，因此注意力机制能有效提高命名实体识别模型性能。Bahdanau等[49]在 2014年首次将注意力机制应用于自然语言处理领域，利用注意力机制对编码－解码网络进行优化，在效果上实现了极大的突破。

注意力机制通常与神经网络结合，可以看作是神经网络输出结果上的加权计算结构。叠加注意力机制帮助神经网络输出标签作进一步有序化调整。注意力机制计算过程为：先将神经网络输出结果和每个相关单元进行相似度计算得到权重系数，再使用softmax 函数对这些权重进行归一化计算，然后该层网络学习权值进行权重加权求和，最后得到调整后的输出标签。

当实体识别模型引入注意力机制后，原本神经网络产生的语义编码向量通过注意力机制变成与词语上下文以及词语位置有关的语义编码向量，其中具体的注意力机制计算结构如图7所示。

![](images/11fcb0635d91e0879efbf06ba69b51985ba8c36697cd6ab4a01155c69723f69f.jpg)  
图7注意力机制结构  
Fig.7 The structure of attention model

该模型可以在利用神经网络上下文信息输出标注序列的情况下，叠加注意力机制帮助神经网络输出标签作进一步有序化调整。在注意力网络结构中，注意力机制可以看作是神经网络输出结果上加入的一层加权计算结构。注意力机制计算主要分为3步：第1步是将神经网络输出结果和每个相关单元进行相似度计算得到权重系数；第2步使用softmax函数对这些权重进行归一化计算；第3步对该层网络学习权值进行权重加权求和，最后得到调整后的输出标签。

![](images/eabab9aec43114f0a86944600ae93792f1ab08de9a1bb63e0de007c06e4f8f15.jpg)

随后，Bharadwaj 等[50]在原始的BiLSTM-CRF模型上加入注意力机制来着重刻画对模型学习更有效的字符，使该模型可以快速地应用于数据稀缺的新语言领域，实现跨语言迁移学习。Rei等[51]在RNN-CRF结构基础上使用注意力机制将词向量与字向量简单拼接改为了权重求和，通过两层传统神经网络隐藏层得到注意力机制层的权重值，能保证模型能够动态地使用字向量和词向量信息。Yin等[52]将Bi-LSTM-CRF神经网络结构与能够直接捕获字符之间依赖关系的自注意机制结合，在输入层构造出基于汉字部首的基本级特征，利用该特征挖掘出单个字符隐藏的语义信息能力，以丰富字符的语义信息，减小中文一词多义带来的不确定性。Li等[53]将BiLSTM网络结构和注意力机制结合的BiLSTM-Att-CRF模型应用到中文电子病历命名实体识别中，通过在BiLSTM层之后加入注意力机制，从不同层次提取句子语义特征，显著改善命名实体识别效果。

注意力层能够利用句子中标签顺序及出现规律进行标签预测,解决了神经网络仅根据单个字向量做标签预测的问题。其优越性在于它能够给各单元分配一个相关性权重，能够显性地分辨哪个词对实体正确表示最为关键，有效地剔除噪音，进一步提升模型效果。

# 3命名实体识别方法对比与展望

# 3.1命名实体识别方法对比

衡量实体识别效果有3种常用的指标：召回率（recall）（用 $R$ 表示）、准确率（precision）（用 $P$ 表示）、 $F _ { 1 }$ 值。召回率是模型正确标注的实体个数占输入实体个数的比值；准确率是模型正确标注的实体个数占输入文本中的实体个数的比值; $F _ { 1 }$ 值用来综合评价系统的性能，等于准确率和召回率的加权几何平均值。三者计算公式如下：

$$
P = \frac { \frac { \hbar \pm \vert \mathcal { H } \vert } { \vert \mathcal { K } - \pm \vert \mathcal { H } \vert } \vert \mathcal { F } \mathcal { H } \vert + \hbar \mathcal { F } \mathcal { H } \vert \widehat { \mathcal { H } } \vert \pm \vert \mathcal { H } \vert  \vert \mathcal { H } \vert \vert \mathcal { F } \mathcal { H } \vert } { \hbar \pm \vert \mathcal { H } \vert } \times 1 0 0 \%
$$

$$
F _ { 1 } = \frac { 2 \cdot P \cdot R } { P + R } { \times } 1 0 0 \%
$$

目前命名实体识别方法一直更新迭代，可以看出，实体识别研究不再仅限于关注最后的准确值，更多的是根据多样化的现实情况研发出对应的解决方式。在识别中具体使用哪种识别方式需要考虑不同命名实体之间的差异性。本文重点分析了深度学习下的神经网络、迁移学习、注意力等主流实体识别模型。与传统的机器学习方法相比，神经网络可以自主提取特征，模型结构设计更加灵活；能充分利用上下文信息，将每个神经单元输出信息赋予不同的权值，提升实体识别效率。但在样本稀缺的情况下，依赖大量数据库的神经网络难以提取文本信息的问题，对此提出了迁移学习模型。迁移学习模型通过挖掘源领域和目标领域的共性实现信息的迁移，完成实体识别。命名实体识别模型对比见表3所列。

![](images/65d03f99fbd47e0c16080c4a1d9b82366c9162e2e350770ad1942da773d248c2.jpg)

表3命名实体识别模型对比  
Table 3 Identification model comparison   

<table><tr><td>项目</td><td>模型</td><td>特点</td></tr><tr><td rowspan="5">单 一 模 型</td><td>基于词典和规则的方法</td><td>准确率高,使用简单；需要人工定制,移 植性差</td></tr><tr><td>基于 HMM的机器学习</td><td>训练效率较高；严格的独立观测假设</td></tr><tr><td>基于MEMM 的机器学习</td><td>特征设计灵活,解决了HMM 输出独立 性的问题；局部归一化,存在标签偏置</td></tr><tr><td>基于 SVM的机器学习</td><td>可以解决高维问题,适用大型特征空间; 对参数和核函数选择要求高</td></tr><tr><td>基于CRF 的机器学习</td><td>可利用上下文信息,全局归一化求得最 优解；收敛速度慢、复杂度高</td></tr><tr><td></td><td>基于 RNN 神经网络的 识别</td><td>网络结构具有记忆性；存在梯度爆炸和 梯度消失的问题</td></tr><tr><td rowspan="5">混 合 模 型</td><td>基于词典和 CRF 结合 的方法</td><td>在单一模型的方法上提高了准确率和 训练速度,适合准确率要求较高的领域</td></tr><tr><td>基于 Bi-LSTM-CRF 的 方法</td><td>双向循环神经网络可以抽取更长的上 下文信息,其特征表达更加丰富，利用</td></tr><tr><td>基于 BI-GRU-CRF 的方 法</td><td>CRF 提升输出标签合理性 GRU 结构比LSTM更简单,减少模型 训练计算量</td></tr><tr><td>基于 Bi-LSTM- ATTENTION 的方法</td><td>通过注意力模型对词语增加权重,提高</td></tr><tr><td>迁移学习模型</td><td>识别准确率 适合样本数据稀缺的情况</td></tr></table>

# 3.2 命名实体识别方法在电力系统的应用展望

随着电网规模逐年扩大，地区管辖范围内的发电站、变电站、输变电线路及其他电力设备的数量越来越多，影响调度计划决策过程的电网运行、系统负荷、母线负荷、电网运行方式、机组发电能力等数据也在不断完善和累积。依赖于传统的人工去管控停电计划已经不适用于电网发展的需要，给电网的运行管理带来一定的安全管理隐患，进而直接影响日常供电的可靠性。因而迫切需要提供一种智能化分析校核的支撑手段，为调度计划安排人员提供决策帮助和数据支持，在众多类型的海量信息中快速定位、准确识别与当前调度计划安排对象相关的决策因子。而支撑智能化校核的基础则是在停电信息与电网设备台账之间建立准确的映射关系，即从停电检修工作票信息中提取出与电网设备台账完全吻合的停电设备。

虽然我国电力系统中的停电计划管理在日计划填报时，与电网设备台账之间已经基于调度运行管理系统（OMS）完成了映射关系的建立，但因为OMS注重于流程管理，从而造成两者之间的映射准确性不高，这一问题在配网计划中体现得更为严重。而针对工作内容更为复杂的周、月度、年度计划管理,停电工作内容一直以自然语言描述，计划管理工作更多依赖人工经验开展，缺乏智能化分析手段。而命名实体识别技术能够实现自然语言中非结构化数据的识别，如利用命名实体识别技术从实际检修工作票数据中识别出电力特定实体，完成停电线路信息标准化构建。

总的来说，停电计划校核是电网日常运行中一个复杂的综合统筹工作，需要兼顾电网多年运行积累和当前电网运行变化。为满足新一代电网运行维护的需求，促进电网向智能化、自动化方向发展，利用命名实体识别技术充分挖掘长久以来积累的以自然语言表达的停电计划数据信息，从中提取出与电网设备台账完全吻合的停电设备是未来智能化分析校核工作的关键和发展趋势。

# 4结语

目前命名实体识别的方法是自然语言处理中的热门研究领域，本文按照逐次递进的顺序对命名实体识别技术进行了分析，从最早的基于词典和规则的方法，发展到基于统计的机器学习，再到如今应用广泛的基于深度学习的方法，以及近期提出的注意力模型和迁移学习模型。最后分别对各类别中的主流模型进行分析和对比总结，同时也展望了命名实体识别技术在电力系统中的应用。通过本文可以看出，命名实体识别是一个充满挑战和创新的领域，无论针对学术研究还是实际应用均还有较多拓展空间。相信随着机器学习的不断发展，将会诞生更多优质可靠的模型来解决命名实体识别存在的问题，进而推进人工智能更深层次的研究。

![](images/1b794837f5821dc048dbb521c10cac19476aeb2f1ab0848b8ee13c3d9a719d97.jpg)

# 参考文献：

[1]奚雪峰,周国栋.面向自然语言处理的深度学习研究[J].自动 化学报,2016,42(10):1445-1465. XI Xuefeng，ZHOU Guodong.A survey on deep learning for natural language processing[J]. Acta Automatica Sinica,2016, 42(10): 1445-1465(in Chinese).   
[2] OTTER D W,MEDINA JR,KALITA JK.A surVey of the usages of deep learning for natural language processing[J]. IEEE Transactions on Neural Networks and Learning Systems,2021, 32(2): 604-624.   
[3] 何玉洁,杜方,史英杰,等.基于深度学习的命名实体识别研究 综述[J/OL].计算机工程与应用：1-17[2021-04-20].http://kns. cnki.net/kcms/detail/11.2127.TP.20210326.0937.002.html. HE Yujie,DU Fang，SHI Yingjie,et al.A survey of named entity recognition based on deep learning[J/OL].Computer Engineering and Applications: 1-17[2021-04-20]. http://kns.cnki. net/kcms/detail/112127.TP.20210326.0937.002.html(in Chinese).   
[4] 王得贤,王素格,裴文生,等.基于JCWA-DLSTM的法律文书 命名实体识别方法[J].中文信息学报,2020,34(10):51-58. WANG Dexian，WANG Suge,PEI Wensheng，et al. Named entity recognition based on JCWA-DLSTM for legal instruments[J]. Journal of Chinese Information Processing, 2020,34(10): 51-58(in Chinese).   
[5]刘浏,王东波.命名实体识别研究综述[J].情报学报,2018, 37(3):329-340. LIU Liu,WANG Dongbo.A review on named entity recognition[J]. Journal of the China Society for Scientific and Technical Information,2018,37(3):329-340(in Chinese).   
[6]陶源,彭艳兵.基于门控CNN-CRF的中文命名实体识别[J].电 子设计工程,2020,28(4):42-46. TAO Yuan, PENG Yanbing. Chinese named entity recognition based on Gated-CNN-CRF[J]. Electronic Design Engineering, 2020,28(4): 42-46(in Chinese).   
[7] 西尔艾力·色提,艾山·吾买尔,王路路,等.结合单词-字符引 导注意力网络的中文旅游文本命名实体识别[J].计算机工程, 2021,47(2):39-45. XIERAILI Seti,AISHAN Wumaier,WANG Lulu,et al. Named entity recognition for Chinese tourism texts combining wordcharacter guided attention network[J]. Computer Engineering, 2021,47(2): 39-45(in Chinese).   
[8] CHINCHOR N.MUC-6 named entity task definition (version 2.1)[C]/Proceedings of the 6th Conference on Message Understanding. Columbia,Maryland,1995.   
[9] GRISHMAN R， SUNDHEIM B. Message understanding conference-6 :A brief history[C]//Proceedings of the 16th International Conference on Computational Linguistics. Stroudsburg: Association for Computational Linguistics,1996 : 466-471.   
[10] CHINCHOR N.MUC-7 named entity task definition[C]// Proceedings of the 7th Message Understanding Conference. Virginia,1997.   
[11]李丹,徐童,郑毅,等.部首感知的中文医疗命名实体识别[J]. 中文信息学报,2020,34(12): 54-64. LI Dan，XU Tong，ZHENG Yi,et al. Radical-aware named entity recognition for Chinese medical records[J]. Journal of Chinese Information Processing,2020,34(12): 54-64(in Chinese).   
[12] 刘宇瀚,刘常健,徐睿峰,等.结合字形特征与迭代学习的金融 领域命名实体识别[J].中文信息学报,2020,34(11):74-83. LIU Yuhan，LIU Changjian，XU Ruifeng，et al. Utilizing glyph feature and iterative learning for named entity recognition in finance text[J]. Journal of Chinese Information Processing, 2020,34(11): 74-83(in Chinese).   
[13]罗凌,杨志豪,宋雅文,等.基于笔画ELMo和多任务学习的中 文电子病历命名实体识别研究[J].计算机学报,2020,43(10): 1943-1957. LUO Ling,YANG Zhihao, SONG Yawen,et al. Chinese clinical named entity recognition based on stroke ELMo and multi-task learning[J]. Chinese Journal of Computers,2020,43(10): 1943- 1957(in Chinese).   
[14] PLETSCHER-FRANKILD S,PALLEJA A,TSAFOU K,et al. DISEASES: Text mining and data integration of disease-gene associations[J].Methods,2015,74:83-89.   
[15]WANG Xiaoyan，JIANG Xingpeng，LIU Mengwen，et al. Bacterial named entity recognition based on dictionary and conditional random field[C]//2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). Kansas City: IEEE,2017:439-444.   
[16] LAFFERTYJD,MCCALLUM A，PEREIRA FC N. Conditional random fields: probabilistic models for segmenting and labeling sequence data[C]// Proceedings of the Eighteenth International Conference on Machine Learning.San Francisco: Morgan Kaufmann Publishers Inc.,2001:282-289.   
[17] LAMPLE G,BALLESTEROS M，SUBRAMANIAN S, et al.Neural architectures for named entity recognition[C]// Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. San Diego: Association for Computational Linguistics,2016 ：260-270.   
[18] YADAV V,BETHARD S.A survey on recent advances in named entity recognition from deep learning models[C]//Proceedings of the 27th International Conference on Computational Linguistics(COLING 2018).Santa Fe: Association for Computational Linguistics,2018 ：2145-2158.   
[19] MERRYTON AR,AUGASTA G.A survey on recent advances in machine learning techniques for fake news detection[J].Test Engineering and Management,2020,83：11572-11582.   
[20] NASIRJA,KHAN O S,VARLAMIS I. Fake news detection: A hybrid CNN-RNN based deep learning approach[J]. International Journal of Information Management Data Insights,2021,1(1): 100007.   
[21] LI Jing,SUN Aixin, HAN Jianglei, et al. A survey on deep learning for named entity recognition[J]. IEEE Transactions on Knowledge and Data Engineering,2020,99 ：1.   
[22] CHIU JP C,NICHOLS E.Named entity recognition with bidirectional LSTM-CNNs[J].Transactions of the Association for Computational Linguistics,2016,4：357-370.   
[23] STRUBELL E,VERGA P,BELANGER D,et al. Fast and accurate entity recognition with iterated dilated convolutions[C]// Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing(EMNLP 2017). Copenhagen: Association for Computational Linguistics,2017 ：2670-2680.   
[24] DAUPHIN Y N, FAN A, AULI M, et al. Language modeling with gated convolutional networks[C]//Proceedings of the 34th International Conference on Machine Learning(ICML 2017). Sydney: JMLR.org,2017 : 933-941.   
[25] ZHAO Lujun，QIU Xipeng，ZHANG Qi,et al. Sequence labeling with deep gated dual path CNN[J].IEEE/ACM Transactions on Audio, Speech,and Language Processing,2019, 27(12): 2326-2335.   
[26] HU J,SHI X,LIU Z,et al. A hybrid system for entity recognition from Chinese clinical text[C]//China Conference on Knowledge Graph and Semantic Computing. Chengdu,2017 : 25-30.   
[27] RIFAT MJR,ABUJAR S,NOORI SRH,et al.Bengali Named Entity Recognition: A survey with deep learning benchmark[C]//2019 10th International Conference on Computing，Communication and Networking Technologies (ICCCNT). Kanpur: IEEE,2019 ：1-5.   
[28] PASCANU R,MIKOLOV T,BENGIO Y.On the dificulty of training recurrent neural networks[C]//Proceedings of the 30th International Conference on Machine Learning(ICML 2013). Atlanta,2013：1310-1318.   
[29] HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J]. Neural Computation,1997,9(8): 1735-1780.   
[30] HABIBI M,WEBER L,NEVES M,et al. Deep learning with word embeddings improves biomedical named entity recognition[J].Bioinformatics，2017,33(14): 37-48.   
[31] WANG Qi, ZENG Lu. Chinese symptom component recognition via bidirectional LSTM-CRF[C]//2018 Tenth International Conference on Advanced Computational Intelligence (ICACI). Xiamen:IEEE,2018:45-50.   
[32] 曾青霞，熊旺平，杜建强，等.结合自注意力的BiLSTM-CRF的 电子病历命名实体识别[J].计算机应用与软件,2021,38(3): 159-162. ZENG Qingxia，XIONG Wangping，DU Jianqiang，et al. Electronic medical record named entity recognition combined with self-attention BILSTM-CRF[J]. Computer Applications and Software,2021,38(3): 159-162(in Chinese).   
[33] LI Weiyan,SONG Wenai, JIA Xinhong,et al. Drug specification named entity recognition base on BiLSTM-CRF model[C]//2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC).Milwaukee:IEEE,2019：429-433.   
[34] 陈茹，卢先领.融合空洞卷积神经网络与层次注意力机制的中 文命名实体识别[J].中文信息学报,2020,34(8):70-77. CHEN Ru,LU Xianling.Combing iterated dilated convolutions neural network and hierarchical attention network for Chinese named entity recognition[J].Journal of Chinese Information Processing,2020,34(8): 70-77(in Chinese).   
[35] JOZEFOWICZ R,ZAREMBA W，SUTSKEVER I.An empirical exploration of recurrent network architectures[C]// Proceedings of the 32nd International Conference on Machine Learning(ICML).Lille,France: JMLR.org,2015 :2342-2350.   
[36]石春丹,秦岭.基于BGRU-CRF的中文命名实体识别方法[J]. 计算机科学,2019,46(9):237-242. SHI Chundan，QIN Ling. Chinese named entity recognition method based on BGRU-CRF[J]. Computer Science,2019,46(9): 237-242(in Chinese).   
[37] YANG Z L, SALAKHUTDINOV R,COHEN W W. Multitask cross-lingual sequence tagging from Scratch[C]//The 5th International Conference on Learning Representations(ICLR 2017).Toulon,France,2017.   
[38] PAN S J,YANG Qiang.A survey on transfer learning[J]. IEEE Transactions on Knowledge and Data Engineering,2010,22(10): 1345-1359.   
[39]庄福振,罗平，何清,等.迁移学习研究进展[J].软件学报，

![](images/31307b52791117380b1c97df3cba70ca23df0e8dfaaa2b572d1c3f1461823cd8.jpg)

![](images/ca772822148d6c96e9fb24cb6c8d00cfb0c9abf79ad5add970b621e33b14e75c.jpg)

![](images/ccfc956e46aaefa4616252bc0cd9e65bd74b1b42dea9dbf2e0853839fc5b4757.jpg)

2015,26(1): 26-39. ZHUANG Fuzhen,LUO Ping，HE Qing，et al.Survey on transfer learning research[J].Journal of Software,2015,26(1): 26-39(in Chinese).   
[40] ZHUANG Fuzhen, LUO Ping,XIONG Hui,et al. Cross-domain learning from multiple sources:a consensus regularization perspective[J]. IEEE Transactions on Knowledge and Data Engineering,2010,22(12): 1664-1678.   
[41] BHATIA P，ARUMAE K,CELIKKAYA B.ToWards fast and unified transfer learning architectures for sequence labeling[C]//2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA). Boca Raton: IEEE,2019：1852-1859.   
[42] KANG M,BISWAS A,KIM D C,et al. Semi-supervised discriminative transfer learning in cross-language text classification[C]/2019 18th IEEE International Conference on Machine Learning And Applications (ICMLA). Boca Raton: IEEE,2019：1031-1038.   
[43] WEI Xiaochi,HUANG Heyan，NIE Liqiang，et al.I Know what you want to express: Sentence element inference by incorporating external knowledge base[J].IEEE Transactions on Knowledge and Data Engineering,2017,29(2): 344-358.   
[44] GLIGIC L,KORMILITZIN A,GOLDBERG P,et al. Named entity recognition in electronic health records using transfer learning bootstrapped neural networks[J]. Neural Networks, 2020,121:132-139.   
[45] XU Jingjing，HE Hangfeng，SUN Xu, et al. Cross-domain and semisupervised named entity recognition in Chinese social media: A unified model[J].IEEE/ACM Transactions on Audio， Speech, and Language Processing,2018,26(11): 2142-2152.   
[46] 武惠，吕立,于碧辉.基于迁移学习和BiLSTM-CRF的中文命 名实体识别[J].小型微型计算机系统,2019,40(6):1142-1147. WU Hui,LV Li,YU Bihui. Chinese named entity recognition based on transfer learning and BiLSTM-CRF[J]. Journal of Chinese Computer Systems,2019,40(6): 1142-1147(in Chinese).   
[47] KIM J,KANG S,PARK Y,et al. Transfer learning from automatically annotated data for recognizing named entities in recent generated texts[C]//2019 IEEE International Conference on Big Data and Smart Computing (BigComp).Kyoto: IEEE, 2019 : 1-5.   
[48] 王银瑞,彭敦陆,陈章,等.Trans-NER:一种迁移学习支持下的 中文命名实体识别模型[J].小型微型计算机系统,2019,40(8): 1622-1626. WANG Yinrui,PENG Dunlu,CHEN Zhang，et al. Trans-NER: a Chinese named entity recognition model supported by transfer learning[J]. Journal of Chinese Computer Systems,2019,40(8): 1622-1626(in Chinese).   
[49] BAHDANAU D,CHO K,BENGIO Y.Neural machine translation by jointly learning to align and translate[J].Computer Science,2014,arXiv:1409.0473.   
[50] BHARADWAJA，MORTENSEND，DYER C，et al. Phonologically aware neural model for named entity recognition in low resource transfer settings[C]/Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing，(EMNLP 2016).Austin: Association for Computational Linguistics,2016 ：1462-1472.   
[51] REI M,CRICHTON G K O,PYYSALO S.Attending to characters in neural sequence labeling models[C]/Proceedings of COLING 2016 The 26th International Conference on Computational Linguistics.Osaka: The COLING 2016 Organizing Committee,2016 ：309-318.   
[52] YIN Mingwang，MOU Chengjie，XIONG Kaineng，et al. Chinese clinical named entity recognition with radical-level feature and self-attention mechanism[J]. Journal of Biomedical Informatics,2019,98：103289.   
[53] LI Luqi，HOU Li.Combined attention mechanism for named entity recognition in Chinese electronic medical records[C]//2019 IEEE International Conference on Healthcare Informatics (ICHI). Xi'an: IEEE,2019：1-2.

![](images/85623e17eaeeb79fddccaa3c924a70b1d2b57eda6233a48d4580f875fee1a3be.jpg)

编辑张京娜收稿日期:2021-03-06

# 作者简介：

江千军(1972-),男，高级工程师，从事电力系统与人工智能方向的研究工作;

桂前进(1975-),男，高级工程师，通信作者,从事电力系统及其自动化方向的研究工作，guiqj1030@ah.sgcc.com.cn;

![](images/2c98c3cabd57a94ef43f94afa162cd9c0ac8555620db196dfdd82a2ee0b3e151.jpg)

江千军 王磊(1976-),男，高级工程师，从事电网运行方向的研究工作;

徐瑞翔(1988-),男，助理工程师，从事电网运行方向的研究工作；

王京景(1983-),男，工程师，从事电网运行分析与控制方向的研究工作;

麦立(1980-),男,工程师,从事电力系统与电网运行方向的研究工作；

许水清(1991-),男，副教授,从事命名实体识别方向的研究工作。