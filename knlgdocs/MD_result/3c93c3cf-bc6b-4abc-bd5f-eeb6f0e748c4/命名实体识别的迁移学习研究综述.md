# 命名实体识别的迁移学习研究综述

李猛,李艳玲+,林民内蒙古师范大学 计算机科学技术学院,呼和浩特 010022$^ +$ 通信作者 E-mail: liyanling7871397@163.com

摘要:命名实体识别(NER)是自然语言处理的核心应用任务之一。传统和深度命名实体识别方法严重依赖于大量具有相同分布的标注训练数据,模型可移植性差。然而在实际应用中数据往往都是小数据、个性化数据，收集足够的训练数据是非常困难的。在命名实体识别中引入迁移学习,利用源域数据和模型完成目标域任务模型构建,提高目标领域的标注数据量和降低目标域模型对标注数据数量的需求,在处理资源匮乏命名实体识别任务上,具有非常好的效果。首先对命名实体识别方法和难点以及迁移学习方法进行概述;然后对近些年应用于命名实体识别的迁移学习方法,包括基于数据迁移学习、基于模型迁移学习和对抗迁移学习,进行全面综述,重点阐述了对抗迁移学习方法;最后进一步思考当前存在的问题并对未来的研究方向进行了展望。

关键词：命名实体识别(NER);迁移学习；对抗迁移学习;深度学习文献标志码:A中图分类号:TP391

# Review of Transfer Learning for Named Entity Recognition

LI Meng, LI Yanling+, LIN Min College of Computer Science and Technology, Inner Mongolia Normal University, Hohhot 010022, China

Abstract: Named entity recognition (NER) is one of the core application tasks of natural language procesing. Traditional and deep NER methods rely heavilyon a large amount of labeled training data with the same distribution,and the portabilityof themodel is very poor.However,data are smalland personalized in practical aplications,and it isverydifficult tocollect enough training data.Transfer learning is used in NER,the dataand model of the sourcedomainare utilized to complete the target task model construction,increase theamountoflabeled data in the target domain and reduce the demand for the amount of labeled data of the target model.It has a very good effect in dealing with the task of low-resource NER.Firstly,the method and diffculty of NER and the method of transfer learning are summarized.Then,transfer learning methods applied to NER including data-based transfer learning,model-based transfer learning and adversarial transfer learning in recent years are comprehensively reviewed, and adversarial transfer learning methods are mainlydescribed.Finally,this paper further expounds thecurrent problems

and looks forward to the future research directions.

Key words: named entity recognition (NER); ransfer learning; adversarial transfer learning; deep learning

命名实体识别(named entity recognition,NER)是自然语言处理(natural language processing,NLP)的一项非常重要的基础任务，旨在自动检测文本中的命名实体并将其分类为预定义实体类型，例如人名、地名、组织机构名等,是人机对话系统、机器翻译、关系抽取等的前置任务[1]。传统以及深度NER[2方法已经取得了非常高的识别精度，但是训练模型需要大规模标注数据,模型性能与标注数据量成正比,在训练语料匮乏的特定领域(如医学、生物等)和小语种(如蒙古语、维吾尔语等)上,性能差强人意。由于训练集和测试集要求独立同分布,因此导致将已有的模型运用到其他领域或者语言上性能差强人意。

迁移学习旨在利用源域中大量标注数据和预训练模型提高目标任务学习性能，凭借其对数据、标注依赖性小和放宽了独立同分布约束条件等优点[7-8],已经成为解决资源匮乏NER的强大工具。NER的迁移学习方法早期工作主要集中在基于数据的方法，利用并行语料库、双语词典等作为桥梁将知识(如标注、特征表示等)从高资源语言投影到低资源语言，主要用于跨语言NER迁移。后来研究者将源模型部分参数或特征表示迁移到目标模型上，不需要额外的对齐信息，实现了跨领域和跨应用NER迁移并取得了非常好的效果。最近NER对抗迁移学习受到越来越多研究人员的关注,引入由生成对抗网络（gene-rativeadversarialnetworks,GAN)[启发的对抗技术,生成一种“域无关特征”,进而实现源域知识到目标域的迁移，帮助目标任务提高学习性能，同时有效缓解了负迁移问题。

刘浏等人[从NER定义、评测会议、主流研究方法等角度,介绍了NER任务的发展历程;Li等人详细地总结和分析了NER的深度学习方法，以及深度NER任务面临的挑战和未来发展方向。Pan和Yang[12]的综述是一项开创性工作，对迁移学习进行了定义和分类，并回顾了2010年之前的研究进展；Weiss 等人[13]介绍并总结了许多同构和异构迁移学习方法；Zhuang等人[8从数据和模型角度对40多种具有代表性的迁移学习模型进行了介绍和对比。以上综述都是对NER或迁移学习单方面的阐述,没有详细地介绍两者结合的方法。本文从基于数据迁移学习、基于模型迁移学习、对抗迁移学习这三方面对目前NER任务的迁移学习方法进行了研究和调查。

# 1命名实体识别

# 1.1传统NER方法

传统的NER方法大致有三类：基于规则、无监督学习和基于特征的有监督学习[1,12]。基于规则的方法依赖语言学家和领域专家手工制定的语义语法规则,通过规则匹配识别各种类型的命名实体，基于规则的方法虽然能够在特定语料上（字典详尽且大小有限)获得很好的效果，但是构建这些规则不仅耗时,难以覆盖所有规则,而且可扩展性和可移植性比较差。无监督学习方法利用在大型语料库上获得的词汇资源、词汇模型和统计信息,使用聚类[1推断命名实体类型。基于特征的有监督学习方法通过监督学习，将NER任务转换为序列标注任务，根据标注数据,将每个训练样本用精心设计的特征表示出来，然后利用机器学习算法训练模型，从看不见的数据中学习相似的模式,其缺点是需要大量人工标注训练数据和人为构造、选择的有效特征[15]。

基于规则的方法主要有LaSIE-ⅡI[]、NetOwl[7]、Facile[18]。基于特征的有监督学习算法主要有隐马尔可夫模型(hiddenMarkov model,HMM)[、决策树(deci-sion trees）[20]、最大熵模型（maximum entropy model,MEM)[21)、支持向量机(support vector machines,SVM)[22]、条件随机场(conditional random fields,CRF)[23]等。

# 1.2深度NER方法

近年来，随着深度神经网络的迅猛发展，NER的深度学习方法已成为主流。深度学习有三个优势：(1)深度神经网络可以进行非线性变换，进而从数据中学习到更复杂的特征;(2)深度学习可以从原始数据中自动学习特征;(3)深度NER模型属于端到端模型[8]。

NER的深度学习方法一般分为三个阶段8,如图1所示，分别是分布式表示、特征提取器和标签解码器。分布式表示把单词或字符映射到低维实值密集向量中，其中每个维度代表一个隐藏特征,它可以自动从文本中学习语义和句法特征，分布式表示有三种：词向量(word embedding)[24]、字符向量(characterembedding)[15]和混合表示[3]。特征提取器通过接收上一层的向量对上下文特征表示进行学习,常用的特征提取器有卷积神经网络（convolutional neural net-works,CNN)[2]、Transformer[25]、循环神经网络（recur-rentneural network,RNN)[2以及它的两种变体门控循环单元(gated recurrent unit,GRU)[27]和长短时记忆网络(long short-term memory,LSTM)[3]。标签解码器是最后一个阶段，以上下文特征为输入，生成标签序列,常见的解码方式有Softmax[24、CRF、RNN。

![](images/30cd928eace89a95351379a0d57cdf0d4e04e9bce45891b334f1276fc21e49a6.jpg)  
Fig.1Deep learning for NER flowchart   
图1NER深度学习流程图

# 1.3深度NER的难点

(1)资源匮乏。深度学习需要大规模标注数据才能很好地训练模型,但数据标注非常耗时且昂贵，尤其对于许多资源匮乏的语言和特定领域，如蒙古语、医学、军事领域。当标注数据较少时，由于无法充分学习隐藏特征，深度学习的性能会大大降低，而且深度学习模型可移植性很差，无法将已有数据和模型应用到资源匮乏领域。因此,采用半监督学习和无监督学习实现资源的自动构建和补足，以及迁移学习等方法都可作为解决该问题的研究方向[28]。

(2)非正式文本。非正式文本即表达不符合书面语法规范的文本，如人机对话系统用户提问、Twitter和微博等社交媒体上的文章评论等用户生成文本，由于其语句简短、口语化、内容宽泛、语意含糊、包含谐音字，使NER变得非常困难,甚至无法识别。例如：“我中奖了"写成“我中了”；“杯具"表示“悲剧”；"xswl"代表"笑死我了”。Li等人使用主动学习将微博文本NER的 $F 1$ 值提高到了 $6 7 . 2 3 \%$ ，但是仍需要人工标记[29]。可以使用注意力机制和迁移学习结合深度学习完成对非正式文本的识别。

(3)命名实体多样性。随着信息化时代的来临，移动互联网的普及，数据规模海量化,命名实体及其类型趋于多样化,同时也在不断演变30。传统的实体类型只有人名、地名和组织机构名，但是现实生活中实体类型复杂多样，不同领域存在不同的实体类型，需要识别更详细的实体类型,例如交通查询领域，需要出发地、目的地、时间、交通工具等实体类型。可以使用迁移学习技术，重复利用已有数据和模型,实现细粒度NER。

(4)命名实体歧义性。自然语言中存在大量歧义问题,这给NER带来很大挑战。在不同文化、背景、领域下，同一实体可能含有不同的含义，例如：“香格里拉"可能是“香格里拉市"也有可能是"香格里拉酒店”。因此需要充分理解上下文语义关系进行识别,可以使用实体链接、注意力机制、特征融合、图神经网络等方法，挖掘更详细、更深层次的语义信息，从而消除命名实体的歧义性[28]。

(5)实体嵌套。实体嵌套指实体内部有一个或多个其他实体,例如：“中国驻俄罗斯大使馆"这一组织机构名中包含了“中国"和"俄罗斯"两个地名。嵌套实体中包含了实体与实体之间丰富的语义关系，充分利用嵌套实体的嵌套信息，可以帮助人们更详细、更深层次地理解文本。Xu等人使用SVM和CNN抽取中文嵌套实体的语义关系[3。Xia等人使用MGNER(multi-grainedNER)模型,重构了命名实体识别的流程,对嵌套实体进行识别[32]。

# 2迁移学习

随着信息化时代的来临，传统机器学习以及深度学习已经取得了巨大的成功，并已经应用到许多实际生活中。但是它们严重依赖于大量具有相同数据分布的标记训练数据,然而实际应用中，收集足够的训练数据是非常困难的。半监督学习、无监督学习可以放宽对大量标注数据的需求进而可以解决部分问题,但是训练的模型性能不尽如人意。迁移学习是机器学习中解决训练数据不足这一基本问题的重要方法，旨在利用来自源域的知识提高目标任务学习性能8。它放宽了机器学习中的两个基本假设：(1)用于学习的训练样本与新测试样本满足独立同分布条件；(2)必须有足够可利用的训练样本才能学习得到一个性能不错的模型[33]。在迁移学习中,给定源域 $D _ { \mathrm { s } }$ (含有大量标注数据,如英语、新闻领域等)和源任务 $T _ { \mathrm { s } }$ ，目标域 $D _ { \mathrm { \tau _ { T } } }$ （只有少量或完全没有标注数据,如蒙古语、社交媒体领域、医学领域等)和目标任务 $T _ { \mathrm { r } }$ ，其中 $D _ { \mathrm { s } } { \neq } D _ { \mathrm { T } }$ 或者 $T _ { \mathrm { s } } { \neq } T _ { \mathrm { T } ^ { \circ } }$ 迁移学习已经广泛地应用于NLP、计算机视觉等领域,是当前机器学习中的研究热点。

# 2.1传统迁移学习方法

传统迁移学习方法分为基于数据和基于模型的方法。基于数据方法主要使用实例加权和特征转换，以减小源域样本和目标域样本之间的分布差异。Dai等人提出了TrAdaboost[34],将Adaboost算法扩展到了迁移学习中,提高可用源域实例权重，降低不可用源域实例权重。Huang等人提出核均值匹配法（kernel mean matching,KMM）,通过再生核希尔伯特空间(reproducing kernel Hilbert space,RKHS)中匹配源域和目标域实例之间的均值，估计源域和目标域概率分布，使得带权源域和目标域概率分布尽可能相近[3]。Pan等人提出迁移成分分析(transfer com-ponent analysis,TCA)[3],采用最大均值误差(maxi-mum meandiscrepancy,MMD)[37作为度量准则测量边缘分布差异,以分散矩阵作为约束条件,将源域和目标域之间的分布差异最小化。

基于模型方法是指利用源域和目标域之间的相似性和相关性,将已训练好的部分源域模型或特征表示迁移到目标模型上，以提高目标模型的性能。Duan等人提出领域自适应机(domain adaptation machine,DAM)通用框架，借助在多个源域上分别预先训练的基本分类器,为目标域构造一个鲁棒的分类器[38]。为了解决分类问题,Tommasi等人提出了一种单模型知识迁移方法（single-model knowledge transfer,SMKL）,该方法基于最小二乘SVM,从源域选择一个预先获得的二进制决策函数,然后迁移其参数到目标模型[39]。Yao等人在TrAdaBoost的基础上做了多源扩展，提出了TaskTrAdaBoost,首先在每个源域上执行AdaBoost构造一组候选分类器,然后每次迭代挑选出在目标域上具有最低分类误差的候选分类器并为其分配权重,最后将所选分类器组合产生最终预测[40]。

# 2.2深度迁移学习方法

深度学习目前是机器学习领域最流行的方法，许多研究者利用深度学习技术构建迁移学习模型，已经成为解决深度学习数据依赖和训练数据不足等问题的重要方法。深度迁移学习分为两类：非对抗方法和对抗方法。

非对抗方法复用在源域中预先训练好的部分深度神经网络,将其迁移至目标模型中。Tzeng等人提出深度域混淆(deep domain confusion,DDC)解决深度网络的自适应问题，在源域与目标域之间添加了一层适应层和MMD,让深度迁移网络在学习如何分类的同时,减小源域实例与目标域实例之间的分布差异[4I。Long等人对DDC进行了扩展，提出了深度自适应网络(deep adaptation network,DAN)[42],在深度神经网络中加入多层适应层和表征能力更好的MK-MMD[43]。Long等人在DAN网络的基础上又提出了联合自适应网络（joint adaptation network,JAN）,相比于DAN只考虑边缘分布自适应，JAN使用效果更好的多层联合分布自适应[44]。

对抗方法引入由GAN网络启发的对抗技术，使模型无法识别特征来自源域还是目标域,进而完成源域知识到目标域的迁移，同时有效缓解了负迁移问题。Ganin等人提出了结构简单的领域对抗神经网络（domain-adversarial neural networks,DANN) [45],其由特征提取器、标签预测器和领域分类器组成，特征提取器的作用类似于GAN网络生成器，其目的是生成"域无关"的特征表示,领域分类器起着类似于判别器的作用，试图检测提取的特征是来自源域还是目标域，通过在领域分类器和特征提取器之间使用对抗技术,学习一种"域无关特征”。Tzeng等人提出了一种对抗领域自适应的通用框架ADDA(adver-sarial discriminative domain adaptation）[46],利用判别模型、无条件权重共享和GAN损失,解决领域之间的数据分布问题。Zhang等人提出了一种部分领域自适应的方法,称为基于重要性加权对抗网络的领域自适应（importance weighted adversarial nets- based domainadaptation,IWANDA)[47],不再使用一个共享特征提取器，而是分别为源域和目标域提供特定域的特征提取器。

# 3命名实体识别的迁移学习方法

# 3.1基于数据迁移学习

NER的基于数据迁移学习方法大多利用额外高资源语言标注数据作为迁移学习的弱监督训练，以对齐信息作为桥梁，如双语词典[48]、并行语料库[49]和单词对齐[50等,将知识(标注、词向量、特征表示等)从高资源语言投影到低资源语言。基于数据方法在跨语言NER中显示出相当大的优越性，但是对高资源语言标注数据和对齐信息的规模和质量非常敏感，并且仅限于跨语言迁移。

# 3.1.1标注和表示投影法

为了提高跨语言NER的性能以及针对目标语言中没有人工标注,Ni等人提出两种弱监督跨语言NER方法[4标注和表示投影法，以及两种共解码方案基于排除-O置信度和基于等级的共解码方案。

标注投影法利用并行语料库、翻译等对齐语料，将高资源语言中的标注迁移到对应目标语言上，并开发了一种独立于语言的数据选择方案，可以从嘈杂的数据中选择高质量标注投影数据。给定目标语言句子 $y$ ，以及质量得分阈值 $q$ 和实体数量阈值 $n$ ,其投影质量得分 $q ( y )$ ，如式(1)所示：

$$
q ( y ) = { \frac { \sum _ { e \in y } { \hat { P } } ( l ^ { \prime } ( e ) | e ) } { n ( y ) } }
$$

式中， $e$ 代表 $y$ 中的每个实体， $\hat { P } ( l ^ { \prime } ( e ) | e )$ 代表 $e$ 用投影标注 $l ^ { \prime } ( e )$ 标记的相对频率， $n ( y )$ 是 $y$ 中的实体总数。数据选择方案必须满足 $q ( y ) \geqslant q \ , \ n ( y ) \geqslant n .$

表示投影法，首先使用以词向量为输入的前馈神经网络模型训练英语NER系统,然后将目标语言的词向量通过线性映射 ${ \pmb { M } } _ { f  { \epsilon } }$ 投影到英语向量空间中;最后使用训练好的英语NER系统对目标语言进行标记。可通过加权最小二乘法得到线性映射${ \pmb { M } } _ { f  e }$ ，如式(2)所示：

$$
{ \pmb M } _ { f  e } = \arg \operatorname* { m i n } _ { { \pmb M } } \sum _ { i = 1 } ^ { n } { \pmb w } _ { i } \| { \pmb u } _ { i } - { \pmb M } { \pmb v } _ { i } \| ^ { 2 }
$$

其中， ${ \pmb w } _ { i }$ 表示训练词典中英语目标语言单词对 $( x _ { i } , y _ { i } )$ 的权重, $\textstyle \pmb { u } _ { i } \setminus \pmb { v } _ { i }$ 分别表示英语单词 $x _ { i }$ 和目标语言单词$y _ { i }$ 的词向量。

共解码方案可以有效地结合两种投影法的输出，提高识别精度。基于排除-O置信度的共解码方案是选择置信度分数较高的标注投影法或表示投影法生成的标签，优先选择一种方法的非O标签（即实体标签)。基于等级的共解码方案，给予标注投影法更高优先级，即组合输出包括标注投影法检测到的所有实体，以及与标注投影法不冲突的所有表示投影法检测到的实体。当标注投影法为一段 $x$ 都生成了O标签，则表示投影法检测到 $x$ 的实体标签不会与标注投影冲突。例如：标注投影法的输出标签序列为(B-PER,O,O,O,O),表示投影法的输出标签序列为(B-ORG,I-ORG,O,B-LOC,I-LOC),那么基于等级的共解码方案合并输出为(B-PER,O,O,B-LOC,I-LOC)。

Ni的贡献在于为标注投影法开发了一种语言无关数据选择方案，以及两种共解码方案,有效地提高了NER的识别精度。两种投影法都具有较高灵活性，但是容易受到双语单词对的对齐准确率和英语NER系统准确率的影响。

# 3.1.2双语词典特征表示迁移法

为了丰富低资源语言的语义表示以及缓解词典外单词问题,Feng等人提出了双语词典特征表示迁移法，将双语词典特征表示和词级实体类型分布特征作为目标NER模型的额外输入，并设计一个词典扩展策略估计词典外单词的特征表示[51]。

双语词典特征表示：根据来自高资源语言的翻译,对每个低资源语言单词的所有翻译词向量使用双向长短时记忆网络（bi-directional long short-termmemory,BiLSTM)或注意力机制提取特征表示 $v e c _ { i }$ 每个翻译项目 $T$ 都由一个或多个高资源语言单词组成,例如：中文单词"美国"有四个英文翻译"America"“United States”"USA"和“The United States of America”。

词典扩展策略,用于估计词典外单词的双语词典特征表示。给定低资源语言单词 $x _ { i }$ 及其对应词向量 ${ \pmb w } _ { i }$ 和双语词典特征表示 $v e c _ { i }$ 。使用线性映射函数,如式(3)所示，作为两个语义空间之间的转换，最小化式(4)以优化映射矩阵 $M$ ,在获得 $M$ 之后，对每个词典外单词 $o _ { i }$ 用式(5)估算其特征表示 $v e o _ { i }$

$$
v e c _ { i } = { M } w _ { i }
$$

$$
\mathit { l o s s } _ { \mathit { M } } = \sum _ { i = 1 } ^ { f } \mathopen { } \mathclose \bgroup \left| \left| \boldsymbol { v } \boldsymbol { e } c _ { i } - \boldsymbol { M } \mathbf { w } _ { i } \aftergroup \egroup \right| \right| _ { 2 }
$$

$$
v e o _ { i } = { \pmb M } o _ { i }
$$

单词实体类型的分布特征是每个单词被标记为每种实体类型的概率。实验中只使用了三种最常见的命名实体类型,即P(人名）L(地名）、O(组织名）以及随机生成一个表示非实体的类型N,因此构造了四个实体类型向量 $\{ E _ { \mathrm { p } } , E _ { \mathrm { 0 } } , E _ { \scriptscriptstyle \mathrm { L } } , E _ { \scriptscriptstyle \mathrm { N } } \} , E _ { j } \in \mathbf { R } ^ { d }$ 。然后，使用标准余弦函数计算低资源词向量 ${ \pmb w } _ { i }$ 与实体类型向量 $E _ { j }$ 之间的语义相关性,如式(6)所示。最后，每个低资源和高资源语言单词都分配有一个维数为4的实体类型分布特征, $e _ { i j } = \{ e _ { \mathrm { p } } , e _ { \mathrm { o } } , e _ { \mathrm { L } } , e _ { \mathrm { N } } \} \mathrm { o }$

$$
e _ { i j } = \frac { { \pmb w } _ { i } ^ { \mathrm { T } } { \cdot } { \pmb E } _ { j } } { | | { \pmb w } _ { i } | | \times | | { \pmb E } _ { j } | | }
$$

最后,将低资源词向量 ${ \pmb w } _ { i }$ 、低资源字符向量 $\mathbf { \boldsymbol { c } } _ { i }$ 、双语词典翻译特征 $v e c _ { i }$ 或 $v e o _ { i }$ 以及实体类型分布特征 $\boldsymbol { e } _ { i j }$ 的连接词向量 $\pmb { W } _ { i } = \{ \pmb { w } _ { i } , \pmb { c } _ { i } , \pmb { v e c } _ { i } , \pmb { e } _ { i j } \}$ 作为BiLSTM-CRF模型的输入。

该方法开创性地使用双语词典特征表示和单词实体类型的分布特征表示，丰富了低资源语言的语义表示,并设计了一种词典扩展策略,有效地缓解了词典外单词问题，在低资源NER性能上取得很大的提升。该方法具有非常好的可扩展性，可以将高资源语言的其他知识(例如：WordNet、知识图谱等)整合到体系结构中，还可以扩展到其他NLP任务（例如：意图识别、情感分析）。

# 3.2基于模型迁移学习

NER的基于模型迁移学习不需要额外的高资源语言对齐信息，主要利用源域和目标域之间的相似性和相关性，将源模型部分参数或特征表示迁移到目标模型,并自适应地调整目标模型[52]。例如：Ando和 Zhang[53提出了一种迁移学习框架,该框架在多个任务之间共享结构参数,并提高了包括NER在内多种任务的性能。Collobert等人[2提出一个独立于任务的卷积神经网络，并采用联合训练将知识从NER和词性标注(part-of-speech tagging,POS)任务迁移到组块识别任务。Wang等人[54利用标签感知MMD完成特征迁移,实现了跨医学专业NER系统。Lin等人[5]在现有的深度迁移神经网络结构上引入单词和句子适应层,弥合两个输入空间之间的间隙,在LSTM和CRF层之间也引入了输出适应层，以捕获两个输出空间中的变化。考虑到目标数据的领域相关性差异,Yang 等人[5受知识蒸馏（knowledge distillation,KD)的启发,提出了一种用于序列标记领域自适应的细粒度知识融合模型,首先对目标域句子和单词的领域相关性进行建模,然后在句子和单词级别上对源域和目标域进行知识融合，有效平衡了目标模型从目标域数据学习和从源模型学习之间的关系。

# 3.2.1基于RNN的迁移学习

RNN及其变体已被大量应用于NER任务，并取得了非常高的识别精度。Yang等人利用神经网络通用性,提出了一种基于RNN的序列标注迁移学习框架(RNN-based transfer learning,RNN-TL)[52],通过源任务和目标任务之间共享模型参数和特征表示，提高目标任务的学习性能。并利用不同级别的共享方案，在一个统一的模型框架下处理跨域、跨应用和跨语言迁移。

该方法开发的三种级别共享方案框架分别为T-A、T-B和T-C,如图2所示。T-A共享所有模型参数，最后在源任务和目标任务的CRF层上执行一个标签映射，应用于两个具有相同标签集领域的跨域迁移。T-B共享CRF层前的所有模型参数,即每个任务单独训练一个CRF,用于两个具有不同标签集领域的跨域迁移和跨应用迁移。T-C只共享字符向量和字符特征表示,用于跨语言迁移,侧重于字母相似的语言(如：英语和西班牙语)。该方法的共享参数由两个任务共同优化，源任务和目标任务具有不同的收敛速度,因此提前停止目标任务。

![](images/d47f4e449a54323e9a133b46cfaa24281d1be345745a94cdc48e10d118703f80.jpg)  
Fig.2RNN-based transfer learning model   
图2基于RNN的迁移学习模型

该迁移方法在一个统一的RNN-CRF框架下，在低资源跨域、跨应用和跨语言的序列标注任务上取得了不错的效果,尤其是在跨领域方面。但是还存在一定不足：跨语言迁移只能是字母相似的语言；对于迁移的参数和特征表示没有进行任何筛选工作，这使得负迁移对模型性能产生消极影响。

# 3.2.2参数和神经适应器迁移

NER的实体类型随时间在不断变化，为了解决目标领域出现新的实体类型而导致重新标注数据和训练模型的问题,Chen等人[57提出了一种解决方案：

在目标模型的输出层添加新的神经元并迁移源模型部分参数,然后在目标数据上进行微调(fine-tuned),此外还设计一种神经适应器学习源数据和目标数据之间的标签分布差异,迁移过程如图3所示。

![](images/c9e66814ccdf08e44ea7b1a51779109eb294aa9db6ca90039fe617f650fdc125.jpg)  
Fig.3Parameters and neural adapter transfer model

在目标模型输出层扩展 $n M$ 个神经元用于学习新实体类型,其中 $n$ 取决于数据集标签格式（例如：如果数据集为BIO格式，则 $n = 2$ ,因为对于每个命名实体类型,将有两种输出标签B-NE和I-NE）, $M$ 是新命名实体类型的数量。迁移源模型参数时，目标模型输出层参数用正态分布 $X { \sim } N ( \mu , \sigma ^ { 2 } )$ 得出的权重进行初始化;其他参数都用源模型中相对应参数进行初始化。

神经适应器使用BiLSTM实现，将源模型输出层输出连接到目标模型相应输出上,作为目标CRF的附加输入。可以为目标模型学习两个任务之间的标签分布差异，以减少数据标签不一致的影响。

该方案使用源模型的参数和神经适应器实现模型迁移，是一种非常简单的迁移方法，解决了目标领域出现新实体类型而导致重新标注数据和训练模型的问题。同时神经适应器可以解决标签不一致，且具有提高迁移模型性能的能力。

# 3.3对抗迁移学习

NER的基于模型迁移学习方法虽然取得了很好的性能，但是还存在以下问题有待解决：(1)没有考虑资源间的差异,强制在语言或领域之间共享模型参数和特征表示；(2)资源数据不平衡，高资源语言或领域的训练集规模通常比低资源训练集规模大得多[58],忽略了领域间的这些差异,导致泛化能力差。因此研究者引入受GAN网络启发的对抗技术，学习一种“域无关特征”,实现源域知识到目标域的迁移，同时有效缓解了负迁移问题。

NER的对抗迁移学习流程如图4所示。对抗鉴别器选择有利于提高目标任务性能的源任务特征，同时防止源任务的特定信息进入共享空间。训练完成之后，对抗鉴别器和共享特征提取器达到平衡：对抗鉴别器无法区分共享特征提取器中的特征表示来自源域还是目标域。但是训练达到这个平衡点需要花费大量时间，还有可能发生模型崩溃。

![](images/b7b9745666a1d31c4e657f60f0b3232d8c67075f3d62643d32f13d0c6a6736f4.jpg)  
图3参数和神经适应器迁移模型  
Fig.4NER adversarial transfer learning flowchart   
图4NER对抗迁移学习流程图

# 3.3.1自注意力机制对抗迁移网络

Cao 等人首次将对抗迁移学习应用于NER任务，提出自注意力机制的中文NER对抗迁移学习模型[59]。充分利用中文分词(Chinese word segmentation,CWS)任务更加丰富的词边界信息，并通过任务鉴别器和对抗损失函数过滤中文分词任务的特有信息，以提高中文NER任务性能。同时在BiLSTM层后加入自注意力机制明确捕获两个字符之间的长距离依赖关系并学习句子内部结构信息。

任务鉴别器,通过Maxpooling层和 softmax层识别特征来自哪个领域，可以表示为式(7）式(8)：

$$
S = \mathrm { M a x p o o l i n g } ( H )
$$

$$
D ( S ; \theta _ { d } ) = \operatorname { s o f t m a x } ( W _ { d } S + b _ { d } )
$$

其中， $H$ 表示共享自注意力的输出， $\theta _ { d }$ 表示任务鉴别器的参数， $\boldsymbol { W } _ { d } \in \mathbf { R } ^ { K \times 2 d _ { h } }$ 和 $b _ { d } \in \mathbf { R } ^ { K }$ 是可训练参数, $K$ 是任务数。

通过引人对抗损失函数 $L _ { \mathrm { A d v } }$ ，如式(9)所示，防止中文分词任务的特定信息进入共享空间。 $L _ { \mathrm { { A d v } } }$ 训练共享特征提取器以产生共享特征，使得任务鉴别器无法可靠地判断特征的领域。通过在softmax层下方添加一个梯度反转层完成minimax优化，使共享BiLSTM生成一个特征表示来误导任务鉴别器。

$$
L _ { \mathrm { _ { A d v } } } = \operatorname* { m i n } _ { \theta _ { s } } \Biggl ( \operatorname* { m a x } _ { \theta _ { d } } \sum _ { k = 1 } ^ { K } \sum _ { i = 1 } ^ { T _ { k } } \log D ( E _ { s } ( x _ { k } ^ { ( i ) } ) ) \Biggr )
$$

式中， $\theta _ { s }$ 表示共享特征提取器可训练参数， $E _ { s }$ 表示共享特征提取器, $T _ { \mathit { k } }$ 是任务 $k$ 训练实例的数量, $x _ { k } ^ { ( i ) }$ 是任务 $k$ 的第 $i$ 个实例。

该模型首次将对抗迁移学习应用于NER任务，在WeiboNER数据集[]和 SighanNER数据集[61]上,将$F 1$ 值分别从BiLSTM-CRF模型的 $5 1 . 0 1 \%$ 和 $8 9 . 1 3 \%$ 提高到了 $5 3 . 0 8 \%$ 和 $9 0 . 6 4 \%$ ,并通过实验验证了迁移学习、对抗训练、自注意力机制各个方法对于模型的有效性。

# 3.3.2双重对抗迁移网络

Zhou等人提出了双重对抗迁移网络(dual adver-sarial transfernetwork,DATNet)[58],在通用深度迁移单元上引入两种对抗学习：一是用广义资源对抗鉴别器（generalized resource-adversarial discriminator,GRAD),解决资源数据不均衡和资源差异问题；二是对抗训练，分别在字符向量和词向量层添加以一个小范数ε为界的扰动，以提高模型的泛化能力和鲁棒性。

DATNet根据特征提取器的差异，有两种体系结构：一是特征提取器有共享BiLSTM和资源相关BiLSTM的DATNet-F；二是特征提取器只有共享BiLSTM而没有资源相关BiLSTM的DATNet-P。

GRAD通过权重 $\alpha$ 平衡高低资源的训练规模差异较大的影响，使源域和目标域中提取的特征表示更加兼容，共享BiLSTM的输出与领域无关，并为每个样本提供自适应权重,从而使模型训练的重点放在困难样本上。为了计算GRAD的损失函数,如式(10)所示，共享BiLSTM的输出序列首先通过自注意力机制编码为单个向量，然后通过线性变换投影到标量 $r$

$$
l _ { \mathrm { G R A D } } = - \sum _ { i } \{ I _ { i \in D _ { \mathrm { s } } } \alpha ( 1 - r _ { i } ) ^ { \gamma } \log r _ { i } +
$$

$$
I _ { i \in D _ { \tau } } ( 1 - \alpha ) r _ { i } ^ { \gamma } \log ( 1 - r _ { i } ) \}
$$

式中， $I _ { i \in D _ { \mathrm { s } } }$ 和 $I _ { i \in D _ { \mathrm { T } } }$ 是标识函数,分别表示特征来自源域还是目标域;参数 $\gamma$ 衡量困难和简单样本损失贡献对比, $( 1 - r _ { i } ) ^ { \gamma } ( \mathbb { E } \chi _ { i } ^ { \gamma } r _ { i } ^ { \gamma } )$ 通过测量预测值与真实标签之间的差异控制各个样本的损失贡献。权重 $\alpha$ 和$( 1 - r _ { i } ) ^ { \gamma } ( \mathbb { E } \chi _ { i } r _ { i } ^ { \gamma } )$ 分别减少了高资源样本和简单样本的损失贡献。

对抗训练就是在原始样本的基础上添加以一个小范数 $\epsilon$ 为界的扰动 $\eta _ { _ x }$ ,计算如式(11)所示：

$$
\eta _ { _ x } = \arg \operatorname* { m a x } _ { \eta : \| \eta \| _ { _ { 2 } } \leqslant \epsilon } \log p ( y | \Theta ; x + \eta )
$$

其中, $\Theta$ 是当前模型参数集。 $\eta _ { _ x }$ 按照文献[62]中的策略,通过如下线性化方法近似估算 $\eta _ { _ x } = \epsilon \frac { g } { | | g | | _ { 2 } }$ ，g=$\nabla _ { { \boldsymbol { x } } } \log { p ( { \boldsymbol { y } } | \theta ; x ) } , \epsilon$ 可在验证集上确定。在每个训练步骤中，由 $\Theta$ 参数化当前模型找到的扰动 $\eta _ { \mathnormal { x } }$ ,并通过xad =x+nx构造一个对抗样本,然后进行原始样本和对抗样本混合训练以提高模型泛化能力。对抗训练损失函数 $l _ { \mathrm { A T } }$ 的计算如式(12)所示：

$$
l _ { _ { \mathrm { A T } } } = \log p ( y | \Theta ; x ) + \log p ( y | \Theta ; x _ { \mathrm { a d v } } )
$$

其中， $\log p ( y | \Theta ; x ) \ : \mathrm { ~ , ~ } \log p ( y | \Theta ; x _ { \mathrm { a d v } } )$ 分别表示原始样本及其对抗样本的损失。DATNet分别在字符级和词级向量层使用对抗训练，可以根据式(11)计算字符向量扰动 $\eta _ { c }$ 、源域词向量扰动 $\eta _ { \scriptscriptstyle { W _ { \mathrm { s } } } }$ 和目标域词向量扰动nw

DATNet很好地解决了表示差异和数据资源不平衡的问题,提高了模型的泛化能力，并在跨语言和跨域NER迁移上取得显著改进。通过实验,DATNet-P架构更适合具有相对更多训练数据的跨语言迁移，而DATNet-F架构更适合具有极低资源和跨域迁移的跨语言迁移。

# 4评价指标及NER方法比较

# 4.1命名实体识别评价指标

目前,NER最常用的评价标准有精确率(Preci-sion）、召回率(Recall)和 $F 1$ 值( $F 1$ -score)等。

精确率，在给定数据集中，标注正确实体数占所有被标注实体数的比例,如式(13)所示：

$$
P r e c i s i o n = \frac { T P } { T P + F P }
$$

召回率，在给定数据集中，标注正确实体数占数据集中所有实体数的比例，如式(14)所示：

Table 1 Statistics of NER datasets   
表1NER数据集统计信息  

<table><tr><td>数据集</td><td>语言</td><td>训练集(实体数)</td><td>验证集(实体数)</td><td>测试集(实体数)</td></tr><tr><td>CoNLL2003</td><td>English</td><td>204 567 (23 499)</td><td>51 578 (5 942)</td><td>46 666 (5 648)</td></tr><tr><td>CoNLL2002</td><td>Spanish</td><td>207 484 (18 797)</td><td>51 645 (4 351)</td><td>52 098 (3 558)</td></tr><tr><td>CoNLL2002</td><td>Dutch</td><td>202 931 (13 344)</td><td>37 761 (2 616)</td><td>68 994 (3 941)</td></tr><tr><td>WNUT-2017</td><td>English</td><td>62 730 (3 160)</td><td>15 733 (1 250)</td><td>23 394 (1 740)</td></tr></table>

$$
R e c a l l = { \frac { T P } { T P + F N } }
$$

$F 1$ 值,同时考虑精确率和召回率，是平衡精确率和召回率的综合指标,如式(15)所示：

$$
F 1 - \mathrm { s c o r e } = 2 \times { \frac { P r e c i s i o n \times R e c a l l } { P r e c i s i o n + R e c a l l } }
$$

其中， $T P$ （true positive)表示真阳性,识别出的正确实体数; $F P$ (false positive)表示假阳性,识别出的错误实体数; $F N$ (false negative)表示假阴性,未被识别出的实体数。

# 4.2迁移学习评价指标

为了验证迁移学习模型的性能，通常是在同一数据集下,将实验模型与深度神经网络模型、其他迁移学习模型的精确率、F1值等进行比较。

# 4.3NER方法性能比较

为了评估NER迁移学习方法的性能，本文以CoNLL2003英语NER数据集[3为源域,CoNLL2002西班牙语和荷兰语NER数据集[64以及WNUT-2017英语TwitterNER数据集[5为目标域进行实验。这些数据集的统计信息如表1所示，使用官方的训练集、验证集和测试集的划分方法。实验中使用30维字符向量、50维词向量，LSTM隐状态的数量设置为100维。

表2是一些NER方法的 $F 1$ 值比较,可以看出双语词典特征表示在跨语言迁移方面取得了非常好的效果，相比于LSTM-CRF在西班牙语和荷兰语上的F1值分别提升了3.01个百分点和6.65个百分点，由于领域之间没有标准的对齐信息，该方法无法进行跨领域迁移;DATNet的两个变体优于其他方法，在3个数据集上都有很大提升,DATNet-P更适合跨域迁移，而DATNet-F在跨语言上更有优势。与多任务学习[相比,迁移学习最关心的是目标任务，而不是同时提升所有的源任务和目标任务。

Table 2 NER method performance comparison(F1 -score)   
表2NER方法性能比较(F1值)  

<table><tr><td>Model</td><td>Spanish</td><td>Dutch</td><td>Twitter</td></tr><tr><td>LSTM-CRF</td><td>83.41</td><td>81.74</td><td>40.42</td></tr><tr><td>双语词典特征表示</td><td>86.42</td><td>88.39</td><td>1</td></tr><tr><td>RNN-TL</td><td>85.77</td><td>85.19</td><td>43.24</td></tr><tr><td>多任务学习</td><td>85.88</td><td>86.55</td><td>45.62</td></tr><tr><td>DATNet-P</td><td>87.04</td><td>87.77</td><td>53.43</td></tr><tr><td>DATNet-F</td><td>88.16</td><td>88.32</td><td>50.85</td></tr></table>

# 5总结与展望

本文主要对应用于NER任务的迁移学习方法从基于数据、基于模型和对抗迁移学习三方面进行了归纳总结。迁移学习对于解决NER任务的资源匮乏、实体类型多样化等问题,取得了非常好的效果。基于数据迁移学习在跨语言迁移任务中取得很大成功，但也仅限于跨语言迁移。基于模型迁移学习不需要额外的高资源语言表示，将源模型的部分参数和特征迁移到目标模型上。对抗迁移学习以其独特的对抗训练思想，生成一种"域无关特征”,实现源域知识到目标域的迁移，帮助目标任务提高学习性能，同时有效缓解了负迁移问题,是目前发展潜力最大的NER迁移学习方法，也是今后的研究重点。

随着对NER迁移学习的深入研究,还有一些新的问题需要解决：

(1)负迁移是迁移学习道路上最大的阻碍，虽然寻找源域和目标域之间相关性衡量标准以及对抗迁移学习可以缓解该问题,但是这些方法都有其自身的局限性。如何更好地解决负迁移问题,有待进一步深入研究。

(2)对于多步传导式迁移学习,如何寻找一个或几个既能照顾到目标域也能照顾到源域的中间领域,帮助相关性不大的两个领域之间实现迁移学习，以达到充分利用已有的大量数据，是迁移学习未来的一个研究方向。

(3)在NER对抗迁移学习中如何构建更加强大的对抗鉴别器，帮助共享特征提取器和对抗鉴别器之间更快达到平衡点，处理多源域NER对抗迁移学习任务，以及更好地解决负迁移问题,是下一步研究的主要工作。

# 参考文献：

[1] YADAV V, BETHARD S.A survey on recent advances in named entity recognition from deep learning models[C]// Proceedings of the 27th International Conference on Computational Linguistics, Santa Fe,Aug 20-26,2018. Stroudsburg: ACL,2018: 2145-2158.   
[2] COLLOBERT R,WESTON J,BOTTOU L,et al. Natural language processng (almost） from scratch[J]. Journal of Machine Learning Research,2011,12: 2493-2537.   
[3] HUANG Z, XU W, YU K. Bidirectional LSTM-CRF models for sequence tagging[J].arXiv:1508.01991,2015.   
[4] LAMPLE G,BALLESTEROS M, SUBRAMANIAN S,et al.Neural architectures for named entity recognition[C]// Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego, Jun 12-17, 2016. Stroudsburg: ACL,2016: 260-270.   
[5] CHIU JP C, NICHOLS E.Named entity recognition with bidirectional LSTM-CNNs[J]. Transactions of the Association for Computational Linguistics,2016,4: 357-370.   
[6] MA X Z, HOVY E. End-to-end sequence labeling via bidirectional LSTM-CNNs-CRF[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,Berlin,Aug 7-12,2016. Stroudsburg:ACL,2016:1064- 1074.   
[7] ZHANG B L, PAN X M, WANG T L, et al. Name tagging for low-resource incident languages based on expectationdriven learning[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego,Jun 12-17,2016. Stroudsburg: ACL,2016: 249-259.   
[8] ZHUANG F Z, QI Z, DUAN K, et al. A comprehensive survey on transfer learning[J].arXiv:1911.02685,2019.   
[9] GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al. Generative adversarial nets[C]//Proceedings of the Annual Conference on Neural Information Processing Systems,Montreal,Dec 8-13,2014.Red Hook:Curran Associates,2014: 2672-2680.   
[10] LIU L,WANG D B.A review on named entity recognition [J].Journal of the China Society for Scientific and Technical Information,2018,37(3): 329-340. 刘浏,王东波.命名实体识别研究综述[J].情报学报, 2018,37(3): 329-340.   
[11] LI J, SUN A,HAN J, et al. A survey on deep learning for named entity recognition[J].arXiv:1812.09449,2018.   
[12] PAN S J,YANG Q.A survey on transfer learning[J].IEEE Transactions on Knowledge and Data Enginering,2009,22(10): 1345-1359.   
[13] WEISS K, KHOSHGOFTAAR T M, WANG D D.A survey of transfer learning[J]. Journal of Big Data,2016,3(1): 9.   
[14] NADEAU D, SEKINE S.A survey of named entity recognition and classification[J]. Computational Linguistics,2007, 30(1): 3-26.   
[15] LI J, SUN A X, JOTY S R. SegBot: a generic neural text segmentation model with pointer network[C]//Proceedings of the 27th International Joint Conference on Artificial Intelligence,Stockholm,Jul 13-19,2018: 4166-4172.   
[16] HUMPHREYS K, GAIZAUSKAS R,AZZAM S,et al. University of Sheffield: description of the LaSIE-II system as used for MUC-7[C]//Proceedings of the 7th Message Understanding Conference,Fairfax,Apr 29-May 1,1998.Stroudsburg: ACL,1998:1-20.   
[17] KRUPKA G R, ISOQUEST K H. Description of the NerOwl extractor system as used for MUC-7[C]/Proceedings of the 7th Message Understanding Conference,Virginia, 2005. Stroudsburg: ACL,2005: 21-28.   
[18] BLACK W J, RINALDI F, MOWATT D. FACILE: description of the NE system used for MUC-7[C]//Proceedings of the 7th Message Understanding Conference,Fairfax,Apr 29- May 1,1998. Stroudsburg: ACL,1998: 1-10.   
[19] EDDY S R.Hidden markov models[J]. Current Opinion in Structural Biology,1996,6(3): 361-365.   
[20] QUINLAN JR. Induction of decision tres[J]. Machine Learming, 1986,1(1): 81-106.   
[21] KAPUR J N. Maximum-entropy models in science and engineering[M].New York: John Wiley& Sons,Inc.,1989.   
[22] SHAWE-TAYLOR J, CRISTIANINI N. Support vector machines:an introduction to support vector machines and other kernel-based learning methods[M]. New York: Cambridge University Press,2000.   
[23] LAFFERTY J,MCCALLUMA,PEREIRAFC N.Conditional random fields: probabilistic models for segmenting and labeling sequence data[Cl//Proceedings of the 18th International Conference on Machine Learning,Williamstown, Jun 28-Jul 1, 2001. San Francisco: Morgan Kaufmann Publishers Inc,2001: 282-289.   
[24] STRUBELL E,VERGA P, BELANGER D, et al. Fast and accurate entity recognition with iterated dilated convolutions [C]/Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Copenhagen, Sep 9-11, 2017. Stroudsburg: ACL,2017: 2670-2680.   
[25] VASWANI A, SHAZEER N, PARMAR N, et al. Attention is all you need[C]//Proceedings of the Annual Conference on Neural Information Processing Systems, Long Beach, Dec 4-9,2017. Red Hook: Curran Associates,2017: 5998-6008.   
[26] SHEN Y Y, YUN H, LIPTON Z C,et al. Deep active learning for named entity recognition[C]//Proceedings of the 2nd Workshop on Representation Learning for NLP, Vancouver, Aug 3,2017. Stroudsburg: ACL,2017: 252-256.   
[27] YANG Z, SALAKHUTDINOV R, COHEN W. Multi-task crosslingual sequence tagging from scratch[J].arXiv:1603.06270, 2016.   
[28] CHEN S D, OUYANG X Y. Overview of named entity recognition technology[J].Radio Communications Technology, 2020, 46(3): 251-260. 陈曙东,欧阳小叶.命名实体识别技术综述[J].无线电通 信技术,2020,46(3):251-260.   
[29] LI G, HUANG YF.An approach to named entity recognition towards micro-blog[J]. Application of Electronic Technique, 2018,44(1): 118-120. 李刚,黄永峰.一种面向微博文本的命名实体识别方法 [J].电子技术应用,2018,44(1):118-120.   
[30] SHENG J. Transfer learning in named entity recognition [D]. Harbin: Harbin Institute of Technology, 2019.   
[31] XU HL, LIYQ, HE Y Q, et al. Research on Chinese nested named entity relation extraction[J].Acta Scientiarum Naturalium Universitatis Pekinensis,2019,55(1): 8-14. 许浩亮,李雁群,何云琪,等.中文嵌套命名实体关系抽取 研究[J].北京大学学报(自然科学版),2019,55(1):8-14.   
[32] XIA C Y, ZHANG C W, YANG T, et al. Multi-grained named entity recognition[C]/Proceedings of the 57th Conference of the Association for Computational Linguistics,Florence, Jul 28-Aug 2,2019. Stroudsburg:ACL,2019: 1430-1440.   
[33] ZHUANG F Z, LUO P, HE Q,et al. Survey on transfer learning research[J]. Journal of Software,2015,26(1): 26-39. 庄福振,罗平,何清,等.迁移学习研究进展[J].软件学报, 2015,26(1): 26-39.   
[34] DAI W Y, YANG Q, XUE G R, et al. Boosting for transfer learning[C]//Proceedings of the 24th International Conference on Machine Learning, Corvalis,Jun 20-24, 2007.New York: ACM,2007: 193-200.   
[35] HUANG JY, GRETTON A J, GRETTON A, et al. Correcting sample selection bias by unlabeled data[C]//Proceedings of the 20th Annual Conference on Neural Information Processing Systems,Vancouver, Dec 4-7,2006. Cambridge: MIT Press, 2006: 601-608.   
[36] PAN S J, TSANG I W, KWOK JT, et al. Domain adaptation via transfer component analysis[C]//Proceedings of the 21st International Joint Conference on Artificial Intelligence,Pasadena, Jul 11-17,2009: 1187-1192.   
[37] BORGWARDT K M, GRETTON A,RASCH M J, et al. Integrating structured biological data by kernel maximum mean discrepancy[J]. Bioinformatics,2006, 22(14): 49-57.   
[38] DUAN L, XU D, TSANG I W, et al. Domain adaptation from multiple sources: a domain-dependent regularization approach [J].IEEE Transactions on Neural Networks,2012,23(3): 504-518.   
[39] TOMMASI T, CAPUTO B.The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories[C]//Proceedings of the British Machine Vision Conference,London,Sep 7-10,2009.British Machine Vision Association,2009: 1-11.   
[40] YAO Y, DORETTO G. Boosting for transfer learning with multiple sources[C]/Proceedings of the 23rd IEEE Conference on Computer Vision and Pattern Recognition, San Francisco, Jun 13-18,2010.Washington: IEEE Computer Society, 2010: 1855-1862.   
[41] TZENG E,HOFFMAN J, ZHANG N,et al. Deep domain confusion: maximizing for domain invariance[J].arXiv: 1412.3474, 2014.   
[42] LONG M S, CAO Y, WANG JM, et al. Learning transferable features with deep adaptation networks[C]//Proceedings of the 32nd International Conference on Machine Learning,Lille, Jul 6-11,2015: 97-105.   
[43] GRETTON A, BORGWARDT K M, RASCH M J, et al. A kernel two-sample test[J]. Journal of Machine Learning Research,2012,13: 723-773.   
[44] LONG M S, ZHU H, WANG JM, et al. Deep transfer learning with joint adaptation networks[C]//Proceedings of the 34th International Conference on Machine Learning,Sydney, Aug 6-11,2017: 2208-2217.   
[45] GANIN Y, USTINOVA E,AJAKAN H, et al. Domain-adversarial training of neural networks[M]//Csurka G. Domain Adaptation in Computer Vision Applications. Berlin, Heidelberg: Springer, 2017.   
[46] TZENG E, HOFFMAN J, SAENKO K, et al. Adversarial discriminative domain adaptation[C]//Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition,Honolulu,Jul 21-26,2017.Washington: IEEE Computer Society,2017: 2962-2971.   
[47] ZHANG J, DING Z, LI W, et al. Importance weighted adversarial nets for partial domain adaptation[C]//Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition,Salt Lake City, Jun 18-22,2018.Washington: IEEE Computer Society,2018: 8156-8164.   
[48] ZIRIKLY A, HAGIWARA M. Cross-lingual transfer of named entity recognizers without parallel corpora[C]//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,Beijing,Jul 26-31,2015. Stroudsburg: ACL,2015: 390-396.   
[49] NI J,DINUG, FLORIANR,et al. Weakly supervised crosslingual named entity recognition via effective annotation and representation projection[C]//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,Vancouver, Jul 30- Aug 4,2017. Stroudsburg:ACL, 2017: 1470-1480.   
[50] YAROWSKY D, NGAI G, WICENTOWSKI R. Inducing multilingual text analysis tools via robust projection across aligned corpora[C]//Proceedings of the lst International Conference on Human Language Technology Research, San Diego, Mar 18-21, 2001. San Francisco: Morgan Kaufmann Publishers Inc,2001: 1-8.   
[51] FENG X C,FENG X C, QIN B, et al. Improving low resource named entity recognition using cross-lingual knowledge transfer[C]/Proceedings of the 27th International Joint Conference on Artificial Intelligence,Stockholm,Jul 13-19,2018:4071- 4077.   
[52] YANG Z L, SALAKHUTDINOV R, COHEN W W, et al. Transfer learning for sequence tagging with hierarchical recurrent networks[C]//Proceedings of the 5th International Conference on Learning Representations, Toulon,Apr 24- 26,2017: 1-10.   
[53] ANDO R K, ZHANG T. A framework for learning predictive structures from multiple tasks and unlabeled data[J]. Journal of Machine Learning Research,2005,6:1817-1853.   
[54] WANG Z H, QU YR, SHEN L H, et al. Label-aware double recognition[C]//Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans, Jun 1-6,2018. Stroudsburg: ACL,2018: 1-15.   
[55] LIN B Y,LU W.Neural adaptation layers for cross-domain named entity recognition[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,Brussels,Oct 31-Nov 4,2018. Stroudsburg:ACL,2018: 2012-2022.   
[56] YANG H Y, HUANG S J, DAI X Y, et al. Fine- grained knowledge fusion for sequence labeling domain adaptation [C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Hong Kong,China, Nov 3-7,2019. Stroudsburg: ACL,2019: 4195-4204.   
[57] CHEN L, MOSCHITTI A. Transfer learning for sequence labeling using source model and target data[J].arXiv:1902.05309, 2019.   
[58] ZHOU JT, ZHANG H, JIN D,et al. Dual adversarial neural transfer for low-resource named entity recognition[C]/Proceedings of the 57th Conference of the Association for Computational Linguistics,Florence,Jul 28-Aug 2,2019. Stroudsburg: ACL,2019: 3461-3471.   
[59] CAO PF, CHEN YB,LIU K,et al.Adversarial transfer learning for Chinese named entity recognition with self-attention mechanism[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Oct 31-Nov 4,2018. Stroudsburg: ACL,2018: 182-192.   
[60] PENG N Y,DREDZE M. Named entity recognition for Chinese social media with jointly trained embeddings[C]// Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,Lisbon, Sep 17-21, 2015. Stroudsburg: ACL,2015: 548-554.   
[61] LEVOW G A. The third international Chinese language processing bakeoff: word segmentation and named entity recognition[C]//Proceedings of the 5th Workshop on Chinese Language Processing, Sydney, Jul 22-23,2006. Stroudsburg: ACL, 2006:108-117.   
[62] GOODFELLOWIJ, SHLENS J, SZEGEDY C, et al. Explaining and harnessing adversarial examples[C]//Proceedings of the 3rd International Conference on Learning Representations, San Diego,May 7-9,2015: 6706414.   
[63] YANG P Y, LIU W, YANG JY H. Positive unlabeled learning

via wrapper-based adaptive sampling[C]//Proceedings of the 26th International Joint Conference on Artificial Intelligence,Melbourne,Aug 19-25,2017:3273-3279.

[64] SANG E F,DE MEULDER F.Introduction to the CoNLL2003 shared task: language-independent named entity recognition[C]//Proceedings of the 7th Conference on Natural Language Learning,Edmonton, May 31-Jun 1,2003. Stroudsburg: ACL,2003:142-147.

[65] ZEMAN D,POPEL M,STRAKA M,et al.CoNLL 2017 shared task:multilingual parsing from raw text to universal dependencies[C]//Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,Vancouver,Aug 3-4,2017. Stroudsburg:ACL,2017: 1-19.

![](images/c8f4594616f697f835ad2b93d4511b828bb2efa24efa655499995a1c806b52b5.jpg)

李猛(1993一)，男，河北张家口人，硕士研究生，主要研究方向为自然语言处理、口语理解。LIMeng,born in 1993,M.S.candidate.His re-search interests include natural language pro-cessing and spoken language understanding.

李艳玲(1978一)，女，内蒙古呼和浩特人，博士，副教授，CCF会员，主要研究方向为自然语言处理、口语理解、机器学习等。

![](images/5a01d35bb4179617f04cf3f17fc11945b1946f4afdefa67de2e8c4673516e35e.jpg)

LI Yanling,born in 1978,Ph.D.,associate professor,member of CCF.Her research interests include natural language processing,spoken language understanding,machine learning,etc.

[66] LIN Y, YANG S Q, STOYANOV V S, et al. A multi-lingual multi-task architecture for low-resource sequence labeling[C]// Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,Melbourne,Jul 15-20,2018. Stroudsburg:ACL,2018:799-809.

![](images/7158f9ad6231c874b5a9bc32efbe10fe70673c65d6f742fbcb33563830cdf777.jpg)

林民(1969一),男，内蒙古呼和浩特人，博士，教 授,主要研究方向为自然语言处理、文本挖掘等。 LIN Min, born in 1969,Ph.D.,professor.His research interests include natural language processing,text mining, etc.

# 北京《计算机工程与应用》期刊有限公司专栏征稿启事

一、征稿期刊：《计算机工程与应用》《计算机科学与探索》

二、征稿范围:面向后摩尔时代人工智能算法、算力问题。重点征集类脑计算、存内计算、感存算融合、AI加速器等算法、体系架构优化及其各应用领域的解决方案。

# 三、投稿方式：

1.采用网上投稿方式,投稿网址：《计算机工程与应用》http://cea.ceaj.org/CN/volumn/home.shtml《计算机科学与探索》http://fcst.ceaj.org/CN/volumn/home.shtml2.投稿时请给编辑部留言"专栏投稿"(以便快速处理)。

# 四、论文投稿要求：

1.投稿内容应突出作者的创新成果，具有较重要的学术价值与推广应用价值，未在国内外公开发行的刊物或会议上发表或宣读过，不存在一稿多投问题，论文不涉密，无抄袭，文责自负；2.稿件按照《计算机工程与应用》或《计算机科学与探索》论文版式要求撰写(根据作者投稿期刊);3.对推荐到期刊的论文，不做录取承诺，由期刊按程序审查后决定是否录取。

# 五、投稿须知：

具体投稿要求可参照期刊网站：《计算机工程与应用》http://cea.ceaj.org/CN/column/column107.shtml《计算机科学与探索》http://fcst.ceaj.org/CN/column/column107.shtml

# 六、联系方式：

征稿编委：北京航空航天大学李洪革教授期刊咨询：《计算机工程与应用》陶小雪副主编联系电话：010-89056055，13901380268